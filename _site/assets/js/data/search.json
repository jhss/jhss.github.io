[
  
  {
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리",
    "url": "/posts/8-bit-matrix-multiplication/",
    "categories": "Model Compression, Quantization",
    "tags": "Model Compression, Quantization",
    "date": "2023-02-12 00:00:00 +0900",
    





    
    "snippet": "1. IntroductionLarge pre-trained 언어모델에 대해서 8-bit quantization을 적용하는 기법들이 많이 연구되었지만, 이런 기법들은 350M 이하 스케일에 대해서만 연구되는 경우가 많았습니다. 이 논문에서는 performance 감소없이 billion 단위에서도 적용 가능한 quantization 기법을 제시합니다.이 ...",
    "content": "1. IntroductionLarge pre-trained 언어모델에 대해서 8-bit quantization을 적용하는 기법들이 많이 연구되었지만, 이런 기법들은 350M 이하 스케일에 대해서만 연구되는 경우가 많았습니다. 이 논문에서는 performance 감소없이 billion 단위에서도 적용 가능한 quantization 기법을 제시합니다.이 논문에서는 파라미터 스케일이 6B정도가 되었을 때 Transformer에만 나타나는 특이한 현상이 있다고 합니다. 트랜스포머 레이어 전체에서 25% 정도에서만 관찰이 된다고 하는데, 특정 차원의 feature 크기가 다른 차원의 feature 크기보다 20배정도 크게 나타난다고 합니다.Transformer에서만 관측되는 특이한 현상을 분석하기 위해서 논문에서는 13B parameter를 가진 모델에 대해서 feature 차원의 크기를 비교했고, 값이 6보다 큰 feature를 outlier로 간주했습니다. 그리고 실험을 할 때 특정 library에 있는 오류로 인한 영향을 없애기 위해서 3개의 software(OpenAI, Fairseq, EleutherAI)에 구현된 GPT-2 model을 모두 사용해서 실험을 진행했습니다.위의 그림 (a)는 모델 파라미터 수를 늘림에따라 outlier로 인해 영향을 받는 sequence의 비율을 측정한 것입니다. 예를들어 sequence의 i번째 단어에 특정 차원에 outlier가 등장했는데, 해당 outlier가 다른 단어에 얼마나 영향을 끼치는지를 측정한 값입니다. 그림을 보면 파라미터 수가 6B에서 6.5B로 넘어갈 때 영향을 받는 비율이 급격히 증가한다는 것을 볼 수 있습니다.위에 그림 (a)에서는 perplexity가 감소할수록 outlier feature의 median magnitude값이 커지는 것을 관찰할 수 있는데, 이로 인해 transformer에 Int8 quantization을 적용했을 때 성능이 좋지 않습니다. 왜냐하면 특정 feature의 값이 커지면 quantization range가 커져서 대부분 quantization bin은 비어있는 상태가 되고, 원래 값이 작았던 feature는 0에 가까운 값으로 quantization이 진행되기 때문에 정보 손실이 많이 발생하게 됩니다. 또한 Int8 quantization 뿐만 아니라 16-bit quantization 방법도 마찬가지로 6.7B scale을 넘어가면 outlier로 인해 잘 작동하지 않을 것이라고 논문에서 주장합니다.또한 그림(b)에서는 perplexity가 감소함에 따라 outlier feature 개수가 증가하는 것을 볼 수 있습니다. 논문에서는 6.7B transformer에서 2,048 sequence 기준으로 150k 정도의 outlier feature를 관측했다고 하는, 이런 outlier들은 6개의 hidden dimension에 집중되어있다고 합니다.이런 outlier는 transformer performance에 큰 영향을 끼치는데, 만약 이 7개의 차원을 제거하면 top-1 softmax probability값이 40%에서 20%로 줄어들고, vadliation perplexity 값이 600~1,000% 증가한다고 합니다. 만약에 임의의 7개의 차원을 제거한다면 top-1 probability는 0.02~0.3% 만큼 감소하고 perplexity는 0.1%만큼 증가한다고 하는데, 이런 실험은 outlier feature가 성능에 얼마나 중요한 역할을 하는지 나타냅니다. 그래서 이런 outlier feature들에 대해서 좀 더 quantization precision을 높이면 large scale transformer에서도 모델의 성능을 유지한 채 quantization 기법을 적용할 수 있게 될거라고 논문에서 주장하고 있습니다.이 논문에서 제시한 기법은 앞서 관측된 outlier를 고려해서 quantization 기법을 수행하는 방법입니다. 방법을 요약하자면 outlier feature 차원에 대해서는 16-bit quantization 기법을 적용하고, 그 외의 차원에 대해서는 8-bit quantization을 적용하는 방법입니다.2. Background2.1. 8-Bit Data Types and QuantizationAbsmax quantization은 입력을 8-bit 범위 \\([-127,127]\\)로 바꾸는 방법인데, 127을 곱하고 입력 tensor의 infinity norm으로 나눠주는 방식으로 다음과 같이 계산합니다. 여기서 \\(\\lfloor \\rceil\\)은 반올림을 나타냅니다.\\[\\mathbf{X}_{i 8}=\\left\\lfloor\\frac{127 \\cdot \\mathbf{X}_{f 16}}{\\max _{i j}\\left(\\left|\\mathbf{X}_{f 16_{i j} \\mid}\\right|\\right)}\\right\\rfloor=\\left\\lfloor\\frac{127}{\\left\\|\\mathbf{X}_{f 16}\\right\\|_{\\infty}} \\mathbf{X}_{f 16}\\right\\rceil=\\left\\lfloor s_{x_{f 16}} \\mathbf{X}_{f 16} \\mid\\right.\\]Zeropoint quantization은 입력 분포를 normalized dynamic range \\(nd_x\\)만큼 scale한 후에 zeropoint \\(zp_x\\)만큼 이동시키는 방법입니다. 입력이 asymmetric distribution인 경우에, Absmax를 사용하면 일부 bit만 사용해서 quantization이 진행되는데, zeropoint 방법을 상요하면 전체 bit를 다 사용해서 입력데이터를 표현할 수 있습니다.3. Int8 Matrix Multiplication at scale단일 scaling constant로 quantization을 수행할 때 문제점은 tensor안에 outlier가 1개라도 존재하면 tensor안에 다른 값들의 quantization precision을 감소시킬 수 있기 때문입니다. 그래서 tensor를 여러 block으로 나누고 각 block마다 다르게 scailing factor를 계산하면 outlier의 영향을 block 안으로 한정시킬 수 있습니다. 이 논문에서는 tensor를 vector 단위로 나눠서 outlier의 영향을 줄였습니다.3.1. Vector-wise QuantizationHidden states \\(\\mathbf{X}_{f 16} \\in \\mathbb{R}^{b \\times h}\\)와 weight matrix \\(\\mathbf{W}_{f 16} \\in \\mathbb{R}^{h \\times o}\\)를 곱하는 상황에서, \\(\\mathbf{X}_{f 16}\\)의 각 row에 다른 scale constant \\(c_{x_{f 16}}\\)를 할당하고, \\(\\mathbf{W}_{f 16}\\)의 각 column에 다른 scale constant \\(\\mathbf{c}_{w_{f 16}}\\)를 할당해서 quantization을 수행하면, dequantize를 할 때 \\(c_{x_{f 16}}\\)와 \\(\\mathbf{c}_{w_{f 16}}\\)의 outer product를 이용할 수 있습니다. 그래서 Vector-wise quantization을 통해 Int8 matrix multiplication을 수행하는 과정을 다음과 같이 나타낼 수 있습니다.\\[\\mathbf{X}_{f 16} \\mathbf{W}_{f 16}=\\mathbf{C}_{f 16} \\approx \\frac{1}{\\mathbf{c}_{x_{f 16}} \\otimes \\mathbf{c}_{w_{f 16}}} \\mathbf{C}_{i 32}=S \\cdot \\mathbf{C}_{i 32}=\\mathbf{S} \\cdot \\mathbf{A}_{i 8} \\mathbf{B}_{i 8}=\\mathbf{S} \\cdot Q\\left(\\mathbf{A}_{f 16}\\right) Q\\left(\\mathbf{B}_{f 16}\\right)\\]16-bit floating point precision을 가진 두 행렬 \\(A, B\\)를 곱할 때, 앞서 말한 vector-wise scale factor를 통해 quantization을 수행해서 int8로 변환하고, 변환한 행렬을 곱하고 다시 scale factor의 outer product로 나눠주면 원래 float 16으로 행렬곱 한 것을 근사하게 됩니다.3.2. The core of LLM.int8(): Mixed-precision Decomposition위에서 large scale transformer의 outlier를 분석한 결과에 따르면, outlier는 특정 feature 차원에 집중되어 있기 때문에 outlier가 없는 차원에만 quantization을 하면 성능을 유지한 채 quantization 기법을 적용할 수 있다고 합니다.그래서 feature 차원 중에서 값이 6.0 이상인 feature의 차원을 \\(O=\\{i \\mid i \\in \\mathbb{Z}, 0 \\leq i \\leq h\\}\\)에 집어넣고, 이 집합에 속한 차원에 대해서는 float 16 행렬곱을 수행하고, 나머지 차원에 대해서는 int8 vector-wise quantization을 적용해서 행렬곱을 수행합니다.\\[\\mathbf{C}_{f 16} \\approx \\sum_{h \\in O} \\mathbf{X}_{f 16}^h \\mathbf{W}_{f 16}^h+\\mathbf{S}_{f 16} \\cdot \\sum_{h \\notin O} \\mathbf{X}_{i 8}^h \\mathbf{W}_{i 8}^h\\]이런 방식이 효과적인 이유는 outlier 차원의 비율이 0.1% 정도이기 때문에, 나머지 99.9%의 차원에 대해서는 메모리 측면에서 효율적인 8-bit 행렬곱을 할 수 있기 떄문입니다.4. Experiment이 논문에서는 2개의 실험을 했는데 large scale transformer에 대해서 language modeling 성능과 zero shot accuracy 성능을 기존의 quantization 기법과 비교했습니다. 먼저 Language Modeling 성능은 파라미터 수를 점점 증가시킴에 따라 perplexity 값을 기존의 quantization 기법과 비교를 하는 방식으로 진행했습니다. 데이터는 C4 corpus validation data를 사용했습니다.실험 결과를 보면 다른 quantization 기법들은 6.7B scale 이후에 fp32와 비교해서 perplexity 값이 커졌다는 결과가 나타나는데, 논문에서 제시한 방법은 fp32와 비교해서도 perplexity 값이 거의 차이가 나지 않는 결과가 나타났습니다.그 다음에 OPT model을 사용해서 zero shot accuracy 성능을 비교했는데, 그림을 보면 스케일이 2.7B보다 작을 때는 기존의 8-bit quantization 기법과 논문에서 제시한 기법이 성능이 비슷하지만, 6.7B 정도 스케일이 되었을 때는 성능 차이가 많이 나는 결과가 나타났습니다.5. Discussion and Limitations이 논문에서는 transfomer의 특징을 분석해서 multi-billion scale에 대해서도 잘 작동하는 Int8 quantization 기법을 제시했습니다. 이 방법의 한계는 Int8 data type에만 적용이 가능하단 점인데, Int8로 굳이 한 이유는 현재 GPU와 TPU가 8-bit 중에서 int type만 지원하기 때문입니다. 현재 GPU와 TPU가 8-bit floating point를 지원하지 않기 때문에, 이 방법은 연구하지 않았다고 합니다.또한 스케일이 175B 까지만 한정되어 있는데, 그 이상 모델이 커지면 다른 추가적인 특성 때문에 논문에서 제시한 quantization 기법이 잘 적용되지 않을 수도 있다고 합니다. 마지막으로 논문에서 제시한 기법은 attention function에는 적용하지 않았는데, 그 이유가 attention function에는 parameter가 포함되지 않아서 quantization을 적용해도 메모리 사용량 감소가 일어나지 않기 때문이라고 합니다."
  },
  
  {
    "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 정리 (Chapter 1 ~ 3)",
    "url": "/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/",
    "categories": "Model Compression, Pruning",
    "tags": "Model Compression, Pruning",
    "date": "2023-01-22 00:00:00 +0900",
    





    
    "snippet": "1. Introduction현대의 딥러닝 모델은 대부분 크기가 커서 메모리를 많이 차지하고 학습과 추론 단계에서 계산량이 많이 필요합니다. 이런 이유 때문에 모델을 경량화 하려는 연구들이 많이 진행 되었고, 그 중 한가지 연구방향이 Sparsification 입니다. Sparsification은 고차원 feature space에서 몇개의 파라미터 값을...",
    "content": "1. Introduction현대의 딥러닝 모델은 대부분 크기가 커서 메모리를 많이 차지하고 학습과 추론 단계에서 계산량이 많이 필요합니다. 이런 이유 때문에 모델을 경량화 하려는 연구들이 많이 진행 되었고, 그 중 한가지 연구방향이 Sparsification 입니다. Sparsification은 고차원 feature space에서 몇개의 파라미터 값을 0으로 만들어서 전체가 아니라 일부만 가지고 학습과 추론을 하는 방법입니다. 이 방법을 통해 모델의 complexity를 낮출 수 있습니다. 본 논문에서는 Sparsification을 통해 모델을 경량화하는 여러가지 방법에 대해 정리했습니다.2. Overview of Sparsity in Deep LearningSparsification을 통해 얻을 수 있는 점은 (1) generalization and robustness 향상 (2) 학습과 inference performance 향상 두 가지가 있습니다.2.1. Generalization일반화 성능은 딥러닝 모델의 가장 중요한 측면 중 하나입니다. 일반화 성능은 학습 때 사용하지 않았던 데이터에 대해서 모델의 성능을 통해 측정이 됩니다.Sparsification을 적용하면 초기에는 sparsification이 학습 노이즈 제거역할을 해서 accuracy가 증가합니다. 모델의 크기를 작게하는 것이 regularizer 역할을 해서 모델이 좀 더 데이터의 일반적인 측면을 학습할 수 있도록 도와주는 역할을 합니다 (위의 그림에서 A 영역에 해당). 점점 더 증가시키면 모델의 performance는 안정화되지만 accuracy가 살짝 감소하는 B영역에 도달하고, 여기서 sparsity를 더 증가시키면 accuracy가 갑자기 많이 감소합니다 (C영역).계산량 관점에서 살펴봤을 때도, 위의 그림과 비슷한 커브가 그려질 것입니다. 초기에 sparsity가 적을 경우에는 sparse structure를 저장하고 sparse computation을 하는데 필요한 오버헤드가 있기 때문에 계산량은 천천히 감소할 것입니다. Sparsity를 중간 정도로 증가시키면 계산량을 많이 감소시킬 수 있고, 최대로 증가시키면 storage와 sparse computation 오버헤드로 인해서 계산량의 변화가 거의 없을 것입니다.2.2 Performance and model storageSparsification을 통해 뉴런을 제거할 때, 어떤 구조적인 방법으로 뉴런 전체를 없애거나 필터를 제거하면, sparse structure를 dense structure로 변환할 수 있습니다. 하지만 구조는 신경쓰지 않고 임의로 제거한다면 남아있는 뉴런의 index를 저장해야 해서 추가적인 storage overhead가 필요합니다.크기가 n인 공간에 m개의 non-zero element를 저장하는 방법은 n개의 bitmap을 사용하는 방법부터 \\(mlog(n)\\) bit를 사용한 absolute coordinate scheme까지 다양한 방법이 있습니다. 그리고 최적의 저장 방법은 sparsity 정도에 따라 달라집니다.만약에 n개의 element를 저장할 수 있는 공간에, 각 element가 k bit인 m개의 element를 저장하는 상황을 가정해보겠습니다. 어떤 전략을 사용할지는 하드웨어적인 요소와 저장해야할 파라미터의 크기에 따라 다르지만, 그림 5에서 대략적으로 sparsity의 강도에 따라 최적의 전략을 표시했습니다.가장 단순한 전략은 element 마다 bit 하나를 사용해서 각 element의 존재여부를 표시하는 bitmap (BM) 방법입니다. BM은 상대적으로 dense한 뉴런을 저장할 때 적합하고 추가적으로 \\(n\\) bit가 필요합니다.그 다음으로 단순한 전략은 0이 아닌 element를 absolute offset과 함께 저장하는 방법인 coordinate offset (COO) 입니다. 이 전략은 sparsity가 엄청 높을 때 가장 효율적인데, 왜냐하면 \\(m\\left\\lceil\\log _2 n\\right\\rceil\\)만큼 추가적인 bit가 필요하기 떄문입니다.이 전략은 runlength encoding (혹은 delta coding으로도 알려졌습니다)으로 확장될 수 있는데, 이 방법은 두 원소의 차이만큼 저장이 됩니다. 만약에 index를 기준으로 원소들을 정렬하고 이웃하는 두 원소의 인덱스 차이의 최대값이 \\(\\hat{d}\\) 라면, \\(m\\left\\lceil\\log _2 \\hat{d}\\right\\rceil\\) bits를 사용해서 sparse matrix를 나타낼 수 있습니다.만약 offset이 변화하는 정도가 매우 크다면, zero-padded delta offset을 사용해서 사용하는 bit 수를 \\(\\left\\lceil\\log _2 \\bar{d}\\right\\rceil\\)으로 줄일 수 있습니다. 여기서 \\(\\bar{d}\\) 는 평균적인 차이를 나타내고, \\(\\bar{d}\\) 보다 멀리 떨어졌을 경우에 0의 값을 추가합니다. 이 방법의 오버헤드는 distance 분포에 따라 달라지고, 패딩이 적게 필요한 경우에 이 전략이 효율적입니다.Sparsity가 높은 행렬에 대해서는 **compressed sparse row (CSR), compressed sparse column (CSC),** 혹은 fiber 기반의 전략들은 행렬과 tensor의 인덱스를 저장합니다. 이런 방법들을 dimension-aware scheme이라고 부르는데,  CSR을 예로 들어서 설명을 해보겠습니다. CSR은 \\(n_c \\times n_r\\) 행렬을 column과 row 배열을 사용해서 나타냅니다. Column 배열은 길이가 m이고 \\(\\left\\lceil\\log _2 n_c\\right\\rceil\\) bits안에 각 값의 column index를 저장하고, row 배열은 길이가 \\(n_r\\)이고 \\(\\left\\lceil\\log _2 m\\right\\rceil\\) bits안에 각 row의 offset을 저장합니다.만약에 \\(n=10^8, k = 8\\)인 상황을 가정해보면 dense representation의 경우 bitmap의 storage overhead가 가장 작을 것입니다. Bitmap은 10~70% 정도의 sparsity에서 효율적이고, delta encoding scheme은 80%보다 큰 sparsity에서 효과적입니다. Offset index 전략이나 dimension-aware 전략은 그 보다 큰 sparsity에서 효율적인데, 실제로 그 정도의 sparsity를 가진 deep learning model이 존재할지는 모르겠습니다.2.3. What can be sparsified?이번 문단에서는 Deep learning 모델에서 어떤 요소에 sparsification을 적용할 수 있는지 정리해보겠습니다. 먼저 여기서는 Model sparsification과 ephemeral sparsification을 구분하겠습니다.Model sparsification은 모델을 변경하는 거고 NAS의 일종으로 여겨집니다. 여기서는 파라미터와 뉴런을 sparse하게 만듭니다. Weight sparsification에도 두 가지 종류가 있는데, 임의의 weight을 sparsify하면 모델의 구조가 사라지게 돼서 남아있는 weight마다 index를 저장해야 합니다. 이러면 index를 저장하는데 저장비용이 들고, dense computation에 최적화된 하드웨어에서는 연산속도가 느릴 가능성이 있습니다. 그래서 structured sparsification을 통해 index storage overhead를 줄이고 연산을 빠르게 수행하는 방법이 많이 연구되었습니다. 이런 방법들은 챕터 3, 4에서 자세히 살펴보겠습니다.Ephemeral sparsification은 어떤 하나의 example을 계산하는 도중에 sparsification을 수행하는 방법입니다. 예를들어 ReLU 혹은 rounding SoftMax는 특정 threshold에 대해서 0을 할당하기 때문에 sparsification 방법입니다.  또 다른 ephemeral sparsification 방법은 gradient 기반 학습과 관련이 있습니다. Back-propagation 과정에서 weight을 업데이트 할 때 부분적으로 바꿔주거나, 그레디언트 값이 커질 때까지 파라미터 update를 지연시키는 방법이 있습니다 (5.3 챕터). 이 방법은 forward 할 때 ephemeral sparsification을 적용하는 것과 비슷한 영향을 주고, 특히 분산 환경에서 많은 성능 향상을 가져왔습니다. 또 다른 방법으로는 conditional computation 방법이 있는데, 이 방법은 모델이 example마다 sparse 계산 경로를 동적으로 결정하는 방법입니다. 이런 방법들에 대해서는 챕터 5에서 살펴보겠습니다.2.4. When to sparsify?Ephemeral sparsity는 example마다 동적으로 파라미터를 업데이트하지만, model sparsity는 주로 NAS 같은 절차를 수행합니다. 주로 pruning schedule을 통해 model sparsity가 이루어지는데, 3가지 종류로 나눌 수 있습니다.2.4.1. Sparsity After TrainingTrain-then-sparsify는 모델을 \\(T\\) iterations 만큼 학습한 후에 sparsification을 적용하고 fine-tuning을 통해 모델의 성능(accuracy)을 향상시키는 과정입니다. 이런 방법들은 inference 하는 동안 모델의 accuracy와 generalization 성능을 향상시키는데 목적을 두고 있습니다.2.4.2 Sparsify During TrainingSparsify-during-training (Sparsification schedule)방법은 학습을 시작하고 모델이 수렴하기 전에 sparsification을 적용하는 방법입니다. 일반적으로 Train-then-sparsify 방법보다 연산량이 적지만, convergence 했을 때 성능이 안 좋은 경우가 있고 하이퍼파라미터에 민감하다고 알려져있습니다. 게다가 dense model 전체를 메모리에 계속 유지해야 하기 때문에, capacity가 작은 장치에서는 사용할 수 없습니다. 어떤 방법들은 weight 혹은 gradient를 pruning 하는 대신에, 학습 가능한 binary mask를 사용해서 pruning을 수행합니다.Sparsification schedule에서 중요한 것은 얼마나 빠르게 많은 뉴런을 제거할 것인지 입니다. Prechelet (1997)은 전체 학습 과정동안 고정된 pruning schedule을 사용하면 모델의 generalization 성능이 많이 떨어질 수 있다는 걸 실험에서 관측했습니다. 그래서 generalization loss를 사용해서 pruning rate를 조절했는데, generalization loss가 커지면 pruning rate를 증가시키는 방식을 사용했습니다. 이 방법을 사용해서 모델의 generalization 성능을 많이 향상시켰습니다.다른 방법으로는 Iterative hard thresholding (IHT) 방법이 있는데, 이 방법은 dense와 sparse 방법이 다음과 같이 반복적으로 적용됩니다.(1) Magnitude를 기준으로 top-k weight을 제거하고, 제거한 sparse network를 s번 만큼 fine-tuning 합니다.(2) Pruned weight를 다시 복원시켜서 dense network를 만들고 d번 만큼 학습합니다.(1), (2)번 과정을 \\(i\\)번동안 반복하는데, (1)번 과정은 네트워크에 regularization을 적용하는 과정이고, (2)번은 더 좋은 representation을 학습하기 위한 방법입니다. 이외에도 dense-sparse-dense 순서로 학습하는 방법을 제시한 연구도 있는데, 이런 방법들은 일반적인 SGD 알고리즘에서 모델의 학습능력 (learnability)을 증가시키는 걸 목표로 하고있습니다.뇌가 나이가 들면서 신경가소성이 감소하는 것처럼, 딥러닝 모델도 학습 초기에 중요한 파라미터나 구조가 결정된다고 주장하는 논문들이 있습니다. 특히 Shwartz-Ziv and Tishby (2017) 에서는 SGD기반 학습 방법이 2페이즈로 나누어진다고 주장합니다.(1) 학습 error를 빠르게 최소화하는 drift phase(2) 내부 representation을 압축하는 diffusion phase이런 이론들은 학회에서 논쟁의 여지가 남아있지만, 많은 실험적인 증거들이 발견되고 있습니다. 예를들어 Michel (2019) 에서는 트랜스포머의 중요한 head가 첫 10 epoch 안에 결정된다는 걸 보여줬고, Ding (2019b) 에서는 학습 과정에서 나중에 제거될 뉴런들이 초기에 발견되었고 이후에 추가된 뉴런이 거의 없다는 것을 실험에서 발견했습니다. 이 논문에서는 이런 현상을 early structure adaptation이라고 부릅니다.You (2020)은 early structure adaption을 이용해서 pruning을 수행했는데, 그 방법은 실제 학습을 진행하기전에 논문에서 제시한 low-cost approximation training 방법을 통해 sparse structure를 발견하고 그 구조를 기준으로 네트워크를 학습합니다. 그리고 이후에 Li (2020)은 초기에 learning rate를 크게 주면 모델이 초기에 sparse structure를 쉽게 학습을 하게 되는데, 이후에 learning rate를 낮춰주면서 모델을 학습했습니다.2.4.3. Sparse TrainingFully-sparse training 방법은 sparse model로 시작해서 학습 과정동안 element를 제거하거나 추가하는 방법입니다. 이런 방법들은 하이퍼 파라미터, 스케쥴링, 초기화 방법을 잘 설정해줘야 하지만, sparse model 형태로 학습을 시작한다는 장점이 있습니다. 여기 논문에서는 static sparsity와 dynamic sparsity를 구분하겠습니다. Static sparsity는 학습 시작전에 pruning을 통해 모델 구조를 고정시키고 학습 과정동안 모델 구조를 변화시키지 않고, dynamic sparsity는 다양한 기준에 따라 학습하는 동안 element를 제거하거나 추가하면서 모델 구조를 변화시킵니다. 자세한건 챕터3과 4에서 살펴보고, 여기서는 static sparsity에 대해서만 살펴보겠습니다.Static sparsity 방법은 네트워크의 학습이 시작되기전에 sparse structure를 결정하는 방법입니다. Liu (2019)은 over-parameterized model을 학습한 후에 pruning을 적용하는 과정이 꼭 필요한지 의문이 들었고, 처음부터 작은 네트워크를 학습하면 어떤지 생각해봤습니다. 그래서 몇가지 실험을 통해 sparse model을 처음부터 학습하면 추가적으로 pruning을 적용하지 않아도 CIFAR-10과 ImageNet에 대해서 성능이 어느정도 확보된다는 것을 보여줬습니다.SNIP에서는 학습전에 데이터를 기반으로 네트워크의 unstructured sparse structure를 찾는 방법을 제시했습니다. 어떤 파라미터가 loss에 끼치는 영향이 작으면 해당 파라미터를 제거하는 방식인데,측정하는 방식은 single batch에 대해 대한 \\(w\\)의 중요도를 \\(I_w^{(1)}=\\left|\\frac{\\partial L}{\\partial w} w\\right|\\) 을 이용해서 계산합니다 (30년전에 Mozer and Smolen-sky가 제시한 방법). 이를통해 중요도가 낮은 파라미터를 제거하고 학습을 시작하는 방식입니다.Wang (2020) 은 sparsity가 매우 큰 상황에서 SNIP 방법이 특정 layer의 거의 모든 뉴런을 제거해서 gradient가 네트워크 전체로 퍼지는 것 (gradient flow)을 방해한다고 주장했습니다. 그래서 sparsity가 높은 상황에서는 임의로 pruning을 하는 것보다 SNIP의 성능이 좋지 않다는 걸 발견했습니다. 이를 해결하기 위해 제시한 방법은 gradient 흐름을 막는 bottleneck (layer의 뉴런이 거의 모든 0인 영역)을 미리 파악해서 해당 bottleneck layer에 있는 뉴런은 제거하지 않는 방법입니다.2.4.4. General Sparse Deep Learning Schedules위의 그림은 pruned network를 학습하는 전반적인 과정을 나타냅니다. 각 step은 생략될 수도 있고 여러번 반복될 수 있습니다. (1)번의 경우에 네트워크 구조를 기술한 정보를 디스크에서 가져오거나 SNIP 같은 방법을 통해 sparse network를 생성할 수 있습니다. (2)번은 파라미터를 임의로 초기화 하거나 pre-trained weight을 사용할 수 있습니다.(3)번은 네트워크가 수렴할 때 까지 학습을 반복하는 것인데, 일반적인 dense network를 학습알고리즘을 사용하거나, regularization 처럼 sparsity를 유도하는 알고리즘도 사용이 가능합니다. (4)는 네트워크 안에 구성요소들을 pruning하거나 regrow하는 단계입니다. 챕터 3, 4에서 자세히 설명할 예정입니다.(5)번은 수렴할 때 까지 네트워크 구조를 고정시키는 과정입니다. 일반적으로 생략되는 단계이지만 모델 정확도를 많이 향상시켜준다고 합니다. (6)번과 (7)번은 학습 과정을 반복하는 것입니다.지금까지 소개한 방법들은 (4)번 단계에서 sparsification을 얼마나 주기적으로 적용할 것인지에 관한 횟수를 설정해줘야 합니다. Jin (2016)에서는 적절히 frequency를 바꿔가면서 pruning을 적용하는 것이 모델 퍼포먼스에 큰 영향을 미친다고 주장했습니다. 하지만 pruning 과정에서 사용되는 hyperparameter (frequency, mini-batch size 등)을 어떻게 최적의 값으로 설정할지에 관한 연구는 많이 진행되지 않았습니다.3. Selecting Candidates for RemovalGale (2019)에서 여러 sparsification 방법을 비교했는데 확실하게 누가 제일 좋다라고 말하기 어렵다고 결론을 지었습니다. 왜냐하면 네트워크 구조, 하이퍼 파라미터, learning rate schedule, task에 따라 성능이 전부 다르기 때문입니다. 그래서 여기서는 어느 방법이 제일 좋은지에 대해 설명하기 보다는, 각 방법의 아이디어와 특정 실험 환경에서 해당 방법이 어떤 성능을 냈는지에 대해 정리했습니다. 다음 그림은 제거할 뉴런을 선택할 때 사용하는 방법들의 개괄적인 요약입니다.3.1. Structured vs Unstructured element removalSection 2.2에서 봤던 것 처럼, unstructured sparse weight에서 0이 아닌 원소를 저장하려면 offset이 필요하고, unstructured weight을 연산에 사용하기 위해서 원래 structure를 고려하는 추가적인 processing 과정이 필요합니다. Structured sparsity는 structure를 고려해서 원소를 저장할 수 있기 때문에, 추가적인 offset이 많이 필요하지 않습니다. 하지만 structured removal은 특정 구조 형태로만 제거가 가능하기 때문에 degree of freedom이 낮아서 모델의 성능이 안좋을 가능성이 있습니다.위의 그림은 unstructured 혹은 특정 구조 형태로 제거할 뉴런을 선택하는 방법들입니다. Block 구조의 경우에는 각 block마다 offset을 한번만 저장하면 되므로 블락의 크기가 B라면 unstructured sparsity와 비교해서 저장공간은 B만큼 줄어들게 됩니다. Strided 구조의 경우에는 첫번째 offset과 stride의 크기, 파라미터 값만 저장하면 weight 전체를 저장할 수 있어서 저장공간이 엄청나게 줄어들게 됩니다. Stride 구조를 사용하는 경우는 주로 channel 단위 sparsification, 연속하는 layer에있는 두 개의 feature 의 연결을 제거, 혹은 특정 stride마다 연속하는 layer의 두 feature 사이에 연결을 제거하는 경우입니다.3.2. Data-free selection based on magnitudeSection 3.1에서 제시한 두 방법은 저장하는 방식과 제거하는 방식에 차이가 있지만, 근본적으로 제거할 뉴런을 선택할 때 고려하는 기준은 유사합니다. 가장 단순하고 효과적인 방법중에 하나는 absolute magnitude가 가장 작은 weight를 제거하는 것입니다. 이 방법은 단일 뉴런에 대해서도 적용이 가능하고, block이나 group 안에 있는 뉴런의 absolute magnitude를 합하는 형태로도 사용이 가능합니다.Weight 값의 분포는 보통 정규분포를 따르기 때문에, magnitude를 기준으로 pruning을 하면 값이 0 근처인 weight를 제거하게 됩니다. 위의 그림에서 (a)는 pruning 이전에 weight 값이 분포, (b)는 \\(|w| \\le x\\)를 기준으로 pruning 이후에 분포, (c)는 retraining 이후에 분포를 나타냅니다. 여기서 중요한 것은 \\(x\\)를 어떻게 선택하는지에 관한 것인데, threshold를 \\(w\\)를 통해 parameterization해서 학습 과정동안 layer마다 학습하는 방법이 있고, reinforcement learner를 사용해서 각 layer마다 적합한 threshold를 찾는 방법이 있습니다.Magnitude pruning 말고도 data 없이 pruning을 수행하는 방법들이 있습니다. Fully connected layer에서 N개의 output이 있는 layer에서 뉴런을 제거하고 싶을 때, N개의 output 뉴런의 input weight의 유사도를 계산해서 N x N 행렬 \\(S\\)를 만듭니다. 예를들어 \\(S_{i,j}\\)는 \\(i\\)번째 output을 생성하기 위해 사용한 weight와 \\(j\\)번째 output을 생성하기 위해 사용한 weight 사이에 유사도를 의미합니다. 이렇게 계산을 하고 유사도가 높은 뉴런 그룹을 만들어서, 그룹안에 대표 1개만 남겨두고 나머지 뉴런은 제거하는 방법입니다. 이 방법은 Srinivas and Babu가 2015년에 제시헀는데, 실험결과를 통해 유사한 뉴런은 불필요하다는 걸 주장합니다. 하지만 이 방법은 크기가 작은 네트워크에 대해서만 잘 적용이 되고, 크기가 커지면 적용이 안되는 현상이 있습니다.Data-free 방법들은 효과적이고 SOTA 결과를 낼 때가 있지만, sparsity가 큰 경우에 좀 더 정확한 방법이 필요하고 pruning 이후에 re-training 과정에서 종종 비용이 많이 발생합니다.3.3. Data-driven selection based on input or output sensitivity, activity, and correlationData를 고려한 selection 방법은 학습 데이터에 대해서 뉴런 output의 sensitivity 혹은 전체 네트워크의 sensitivity를 고려합니다. 뉴런의 값이 0에 가까우면 학습 데이터가 변해도 전체 네트워크 output에는 많이 영향을 끼치지 않기 때문에 sensitivity가 낮을 것입니다. 이런 이유로 인해서 sensitivity를 측정해서 제거할 뉴런을 선택합니다.가장 단순한 방법으로는 전체 입력 데이터에 대해서 output에 변화가 거의 없는 뉴런의 경우, 해당 뉴런을 제거하고 일정한 상수값을 가진 bias로 대체하는 방법입니다. 이 방법을 좀 더 확장해서, 각 layer의 input이 변할 때 output이 거의 변하지 않는 뉴런을 삭제하는 방법도 있습니다. Han and Qiao (2013)는 FFT를 이용해서 입력 변화에 대한 출력 변화도를 측정했습니다.다른 방법으로는 항상 같이 활성화되는 output을 제거하는 방법이 있습니다. 전체 학습 데이터에 대해서 대부분 output이 유사하면, 대표 뉴런 1개만 남겨두고 나머지 뉴런을 제거하는 방식으로 pruning이 수행됩니다.마지막으로 연결된 뉴런 사이의 connection 강도를 측정해서, 약하게 연결된 뉴런의 weight는 제거하고 강하게 연결된 뉴런의 weight를 유지하는 방법이 있습니다.3.4. Selection based on 1st order Taylor expansion뉴럴네트워크를 학습할 때 gradient를 기반으로 학습하기 때문에 gradient를 통해 제거할 뉴런을 선택하면 계산비용을 줄일 수 있습니다. 보통 특정 weight 값이 변했을 때 loss값이 많이 변하지 않으면, 해당 뉴런이 중요하지 않다고 판단해서 제거를 합니다. 이때 weight에 대한 loss \\(L(\\mathbf{w})\\)의 변화를 계산할 때 다음과 같이 근사할 수 있습니다.\\[\\delta L=L(\\mathbf{w}+\\delta \\mathbf{w})-L(\\mathbf{w}) \\approx \\nabla_{\\mathbf{w}} L \\delta \\mathbf{w}+\\frac{1}{2} \\delta \\mathbf{w}^{\\top} \\mathbf{H} \\delta \\mathbf{w}\\]여기서 \\(\\nabla_{\\mathbf{w}} L\\)  는 gradient, \\(\\mathbf{H}\\)는 Hessian matrix입니다. Gradient만 사용해서 근사할 수도 있고, Hessian matrix까지 사용해서 더 정확하게 계산할 수도 있습니다.변화를 계산할 때 가장 단순한 방법으로는 전체 학습 데이터에 대해서 gradient를 모두 더해서 변화가 적은 weight를 제거하는 방법입니다. 제거할 때는 각 weight마다 binary gating function을 사용합니다. 만약에 \\(\\alpha_i\\)가 \\(i\\)번째 뉴런의 gate라고 하면 forward하는 방식을 \\(f_l=\\sigma_R\\left(W_l \\cdot \\alpha \\odot f_{l-1}\\right)\\) 으로 나타낼 수 있습니다. 이전 layer의 activation \\(f_{l-1}\\) 차원만큼 gate vector \\(\\alpha\\)를 생성해서, 이전 layer의 output 중 어느 것을 현재 layer에 반영할지 선택을 한다는 의미입니다. Loss \\(L\\)에 대한 gate의 변화 \\(\\frac{\\partial L}{\\partial \\alpha_i}\\)가 크다면 해당 뉴런은 중요하고 작으면 제거해도 영향이 없다는 가정하에 pruning을 진행합니다.Xu and Ho (2006)은 Jacobian matrix를 이용해서 제거할 뉴런을 선택하는 방법을 제시했습니다. Jacobian은 보통 full rank가 아니기 때문에 어떤 weight의 gradient는 서로 연관되어 있습니다. Jacobian matrix를 QR 분해를 해서 연관성이 있는 gradient를 제거하는 방법입니다.3.5. Selection based on 2nd order Taylor expansionLe Cun (1990)은 완전히 학습된 모델이 주어졌을 때, 중요하지 않은 weight를 선택하는 문제를 최적화 관점에서 접근했습니다.\\[\\delta L=L(\\mathbf{w}+\\delta \\mathbf{w})-L(\\mathbf{w}) \\approx \\nabla_{\\mathbf{w}} L \\delta \\mathbf{w}+\\frac{1}{2} \\delta \\mathbf{w}^{\\top} \\mathbf{H} \\delta \\mathbf{w}\\]위의 식에서 perturbation \\(\\delta \\mathbf{w}\\)는 \\(i\\)번쨰 뉴런을 0으로 만드는 방향으로 변한다고 가정하면 \\(\\delta \\mathbf{w}=\\left(0, \\ldots,-\\mathbf{w}_i, \\ldots, 0\\right)\\)로 나타낼 수 있습니다. 완전히 학습된 모델이라는 가정이 있기 때문에, model이 local minimum에 있어서 gradient \\(\\nabla_wL=0\\) 입니다. 그러므로 perturbation에 대한 loss의 변화는 다음과 같이 Hessian matrix를 사용해서 나타낼 수 있습니다.\\[\\frac{1}{2} \\delta \\mathbf{w}_i^{\\top} \\mathbf{H} \\delta \\mathbf{w}_i\\]이 값을 최소화하는 \\(\\mathbf{w}_i\\)를 찾는 최적화 문제를 Lagrange multiplier를 사용해서 풀면 다음과 같은 saliency measure를 얻을 수 있습니다.\\[\\rho_i=\\frac{\\mathbf{w}i^2}{2\\left[\\mathbf{H}^{-1}\\right]{i i}}\\]여기서 \\(\\left[\\mathbf{H}^{-1}\\right]_{i i}\\)는 inverse Hessian matrix의 \\(i\\)번째 diagonal element입니다. 이 값을 내림차순으로 정렬해서 가장 낮은 weight를 제거하는 방법인데, 이 방법을 Optimal Brain Damage (OBD)라고 합니다.위에 식에서 Hessian matrix가 identity인 경우에, weight의 magnitude가 가장 작은 weight를 제거하는 형태로 바뀌게 됩니다. 그래서 Magnitude pruning은 OBD의 특별한 경우라고 해석할 수 있습니다.3.5.1. Discussion of assumptions and guranteesOBD pruning은 수학적 기반이 있지만 몇가지 단점이 있습니다.(1) 완전히 학습된 모델에 대해서 pruning을 수행한다는 가정이 필요합니다. Singh and Alistarh (2020)는 학습이 잘 되지 않은 network (\\(\\nabla_\\mathbf{w}L \\neq 0\\))에 대해서 OBS를 확장했습니다.(2) Saliency measure를 계산할 때 inverted Hessian matrix를 사용하는데, 이게 가능한 이유는 pruning을 수행하는 시점에서 해당 point에 대한 Hessian matrix가 invertible 하다는 가정을 했기 떄문입니다. 그리고 pruning perturbation이 작아서 perturbatino 방향으로 Hessian matrix가 상수라는 가정이 필요한데, 왜냐하면 이 가정이 없으면 loss를 근사할 때 hessian matrix보다 더 높은 차수를 사용해야 하기 떄문입니다.(3) single weight를 제거할 떄 마다 Hessian을 다시 계산해야하기 때문에, 최근에 연구되는 커다란 모델에서는 적용할 수 없습니다. 그리고 Le Cun  (1990)이 제시한 방법에서 Hessian matrix가 diagonal이라는 가정하에 pruning 방법을 제시했는데, 이후에 Hassibi and Stork (1992)는 diagonal 가정을 없애고 다른 가정을 추가해서 inverse Hessian을 추정하는 numerical method를 제시했습니다.3.5.2. Large-scale pruning based on second-order informationSecond-order pruning 기법에서 고려해야할 중요한 사항은 large scale network에 대해서 적용이 가능한지 여부입니다. 왜냐하면 파라미터 크기가 클 때 inverse Hessian matrix의 diagonal 성분을 계산하기 어렵고, Hessian matrix가 non-invertible 가능성도 있기 때문입니다. 이런 문제들을 해결하기 위해 OBD를 확장하는 방법들이 연구되었습니다.The Empirical Fisher Approximation to the Hessian (OBS) 방법은 Hessian matrix를 Empirical Fisher matrix를 이용해서 근사하는 방법인데, Hassibi and Stork (OBS)가 1992년에 가장 처음으로 제시했습니다. 근사할 때 몇가지 가정이 필요한데 softmax를 사용하는 classification task의 경우에만 적용이 가능하고, 학습이 잘 돼서 모델의 output 분포가 true output distribution을 근사한 상태여야 합니다. 두가지 가정하에 Hessian matrix를 다음과 같이 empirical Fisher를 사용해서 근사할 수 있습니다.\\[H \\simeq \\frac{1}{N} \\sum_{j=1}^N \\nabla \\ell_j \\cdot \\nabla \\ell_j^{\\top}\\]Approaches based on low-rank inversion 방법은 Sherman Morrison formula를 응용한 방법입니다. Sherman Morrison formula는 어떤 행렬 \\(A\\)의 inverse를 알고 있을 때, 임의의 column vector \\(u, v\\)에 대하여 \\((A + uv^T)^{-1}\\)를 적은 계산비용으로 계산할 수 있는 방법입니다. 이 방법을 응용해서 \\(j\\)번째 inverse Hessian matrix와 \\(j+1\\)번째 gradient를 알고있을 때, \\(j+1\\)번째 inverse Hessian matrix를 다음과 같이 적은 비용으로 근사할 수 있습니다.\\[\\widehat{H}_{j+1}^{-1}=\\widehat{H}j^{-1}-\\frac{\\widehat{H}j^{-1} \\nabla \\ell{j+1} \\nabla \\ell{j+1}^{\\top} \\widehat{H}j^{-1}}{N+\\nabla \\ell{j+1}^{\\top} \\widehat{H}j^{-1} \\nabla \\ell{j+1}}\\]초기에 \\(\\hat{H_0}^{-1} = \\lambda I_d\\) 이고, \\(\\lambda\\)는 작은 값입니다. 이 접근 방법은 OBS에서 small scale에 적용되었는데, 2020년에 Singh and Alistarh가 이 방법을 기반으로 large scale network에 pruning을 적용하는 방법을 제시했습니다. 위의 방법을 사용해서 block-diagonal approximation을 통해 Hessian matrix를 근사했습니다. 이 방법은 unstructured pruning 기준으로 magnitude 방법이나 diagonal Fihser 방법에 비해서 SOTA accuracy를 달성했습니다.OBD/OBS를 확장하는 방법도 많이 연구되었는데, loss를 기준으로 파라미터의 중요도를 계산하는 게 아니라 generalization error를 기준으로 중요도를 계산하는 방법이 있고, OBS에 weight decay를 추가한 방법, 특정 weight를 제거했을 때 해당 weight와 연관된 모든 weight를 제거하는 방법 등이 있습니다.3.6. Selection based on regularization of the loss during trainingCost function에 penalty term을 추가하는 regularization을 이용해서 sparsification을 적용하는 방법이 있습니다. Penalty term을 통해 weight가 sparse 형태로 변하고, 모델의 complexity를 낮추게 됩니다. 하지만 penalty term을 추가하면 local minima가 추가적으로 생겨서 최적화된 파라미터 값을 찾기가 더 어려워집니다.3.6.1 \\(L_0\\) normSparse weight를 생성하는 가장 명확한 방법은 weight의 \\(L_0\\) norm을 penalty term으로 사용하는 것입니다.\\[P(\\mathbf{w})=\\alpha\\|\\mathbf{w}\\|_0=\\alpha \\sum_j \\begin{cases}0 &amp; w_i=0 \\\\ 1 &amp; w_i \\neq 0\\end{cases}\\]\\(L_0\\) norm은 파라미터 안에 0이 아닌 원소의 개수를 세는 metric인데, discrete한 특성 때문에 미분이 불가능해서 직접적으로 최적화하기가 어렵습니다. 이 문제를 해결하는 가장 단순한 방법은 Straight-through estimators (Bengio, 2013)을 사용해서 backpropagation 과정에서 미분이 불가능한 point를 identity function으로 대체하는 방법입니다. 몇몇 사람들은 이 방법이 불안정하다고 해서 Softplus 혹은 Leaky ReLU를 사용해서 미분 불가능한 point를 근사했습니다.미분 불가능한 point를 근사하는 두번째 방법으로는 parameterizable continuous function을 사용하는 것입니다. Luo and Wu (2019)는 sigmoid function을 사용해서 Heaviside step function을 근사했습니다. 그림 13 (b)에서 \\(\\beta\\)값이 커지면 step function에 가까워지지만 학습하기가 어려워집니다.마지막으로 magnitude pruning 할 때 threshold를 직접 학습하는 방법이 있습니다. Manessi (2018)는 threshold linear function을 다음과 같이 정의했습니다.\\[v^\\beta(x, t)=\\operatorname{ReLU}(x-t)+t \\sigma(\\beta(x-t))-\\operatorname{ReLU}(-x-t)-t \\sigma(\\beta(-x-t))\\]여기서 \\(t\\)는 threshold parameter이고 \\(\\beta\\)를 조절해서 threshold 바깥에 있는 point의 기울기의 sharpness를 결정합니다. \\(t\\)안에 있는 입력은 0에 가깝게 만들고, \\(t\\) 바깥에 있는 값은 선형함수에 가깝게 근사하는 방법입니다.\\(L_0\\) norm 과 관련있는 다른 pruning 방법으로는 Polarization (Zhuang et al., 2020)이 있는데, 이 방법은 다음과 같은 regularizer를 사용해서 파라미터에서 몇개의 element는 0에 가깝게 만들고, 다른 element는 0에서 멀어지게 만드는 방법입니다.\\[R(\\alpha)=t\\|\\alpha\\|_1-\\left\\|\\alpha-\\bar{\\alpha} 1_n\\right\\|_1=\\sum_{i=1}^n t|\\alpha_i|-|\\alpha_i-\\hat{\\alpha}|\\]여기서 \\(\\bar{\\alpha}=\\frac{1}{n} \\sum_{i=1}^n \\alpha_i\\)인데,  \\(\\left\\|\\alpha-\\bar{\\alpha} \\mathbf{1}_n\\right\\|_1\\) 이 값은 \\(\\alpha_i\\)가 모두 동일할 때 최대가 되고, 절반이 0이고 나머지 절반 값이 같을 때 최대가 됩니다. 이런 성질로 인해서 값이 큰 weight과 작은 weight을 분리시켜주는 역할을 합니다.3.6.2. \\(L_1\\) normLasso라고도 알려진 \\(L_1\\) norm은 \\(L_0\\) norm의 tightest convex relaxation 형태입니다. \\(L_0\\) norm과는 다르게 penaty function이 선형이어서 미분이 가능합니다.\\[P(\\mathbf{w})=\\alpha\\|\\mathbf{w}\\|_1=\\alpha \\sum_i\\left|w_i\\right|\\]\\(L_1\\) penatly는 직접적으로 weight를 0으로 만들지 않고 값을 작게 만들기 때문에 magnitude-based 방법과 같이 사용이 됩니다. \\(L_1\\) 기반 방법의 단점은 \\(L_1\\) norm이 scale에 variant하기 때문에, 모든 파라미터를 같은 속도로 감소시킨다는 것입니다. 이러한 이유로 \\(L_1\\) norm이 효율적인 sparsity 방법이 아니어서 Yang (2020)은 Hoyer regularizer를 사용해서 \\(L_1\\) norm의 단점을 보완하는 방법을 제시했습니다.\\[H_S(\\mathbf{w})=\\frac{\\left(\\sum_i\\left|w_i\\right|\\right)^2}{\\sum_i w_i^2}\\]Hoyer regularizer는 \\(L_1\\) norm과 \\(L_2\\) norm의 비율로 구성돼서 scale-invariant한 penalty term 역할을 합니다. 논문에서는 Hoyer regularizer를 사용해서 같은 accuracy를 기준으로 sparsity가 더 큰 모델을 만들었습니다.3.6.3. Grouped RegularizationGroup lasso는 같은 group안에 있는 변수들을 모두 0으로 혹은 0이 아닌 값으로 만드는 방법입니다. \\(N\\)개의 element를 \\(G\\)개의 그룹으로 나누고 \\(X_g\\)가 각 group의 원소를 포함하고 있다면, group lasso는 다음과 같은 최적화 문제를 푸는 형태로 정의가 됩니다.\\[\\min_{\\beta \\in \\mathbb{R}^p}\\left(\\left\\|\\mathbf{y}-\\sum_{g=1}^G \\mathbf{X}_{\\mathrm{g}} \\beta_g\\right\\|_2^2+\\lambda \\sum_{g=1}^G \\sqrt{n_g}\\left\\|\\beta_g\\right\\|_2\\right)\\]그룹은 보통 특정 뉴런과 연결된 모든 output으로 설정하거나, CNN의 경우는 필터 혹은 채널, 아니면 전체 layer에 대해서 설정할 수도 있습니다.3.7. Variational selection schemesPruning을 적용할 뉴런을 선택하는 다른 방법에는 Bayesian 접근 방식이 있습니다. 여기서는 만약에 특정 뉴런의 variance가 크다면 네트워크 성능에 기여하는 부분이 적다고 가정하고 해당 뉴런을 제거하는 방식입니다. 대표적인 예시로 Sparse Variational Dropout (Sparse VD) (Molchanov, 2017) 방법이 있는데, 이 방법은 각 weight를 정규분포 형태로 parameterization 해서 학습하는 동안 해당 파라미터를 학습합니다.\\(\\mathbf{w} \\sim \\mathcal{N}\\left(\\mathbf{w} \\mid \\theta, \\alpha \\cdot \\theta^2\\right)\\)라고 한다면 \\((\\theta, \\alpha)\\)를 학습하는데, 여기서 \\(\\alpha\\) 값이 크다면 test할 때 해당 뉴런을 0으로 설정해주는 방식입니다. 직관적으로 보면 \\(\\alpha\\)가 크다는건 해당 뉴런에 noise가 많아서 network의 performance에 나쁜 영향을 준다는 것입니다. 이 방법의 장점은 추가적인 hyperparameter tuning 과정이 필요없고, 방법이 비교적 단순하다는 점입니다.하지만 각 weight당 2개의 파라미터가 있기 때문에, 모델 사이즈가 2배로 커진다는 단점이 있고, 아예 처음부터 Sparse VD로 학습하는게 어려워서, pre-trained model에 적용을 하거나 다른 추가적인 기법을 사용해서 학습을 보완해야 합니다.Sparse VD 방법을 통해 sparse neural network를 만들었지만, unstructured 형태로 pruning이 진행되기 때문에 inference 속도에는 거의 변화가 없습니다. 그래서 structured 형태로 variational dropout을 적용하는 Struc-tured Bayesian pruning 기법이 있습니다. Neklyudov (2017)은 각 파라미터를 log-normal distribution 형태로 표현하고, 각 group마다 variational parameter를 공유하는 형태로 모델을 설계했습니다. log-normal noise는 항상 non-negative 여서 입력의 부호를 바꾸지 않고, closed-form 형태로 variational lower bound를 표현할 수 있다는 장점이 있습니다. 뉴런을 제거할 때는 뉴런마다 signal-to-noise ratio (SNR)을 계산해서 SNR이 낮은 그룹을 제거했습니다. 이 방법은 CIFAR-10 이나 MNIST같은 작은 데이터셋에 대해서 가속이 잘 되었습니다."
  },
  
  {
    "title": "A Comprehensive Survey on Graph Neural Networks 정리",
    "url": "/posts/A-Comprehensive-Survey-on-Graph-Neural-Networks/",
    "categories": "Graph Neural Network",
    "tags": "Graph Neural Network",
    "date": "2023-01-08 00:00:00 +0900",
    





    
    "snippet": "1. Introduction딥러닝은 Euclidean space에서 표현된 데이터에 대해서 성공적인 성과를 거두었지만, 최근에 non-Euclidean space에서 생성된 데이터에 딥러닝을 적용하려는 시도가 많아지고 있습니다. 본 논문에서는 GNN을 4개의 카테고리 (Recurrent GNN, Convolutional GNN, Graph autoen...",
    "content": "1. Introduction딥러닝은 Euclidean space에서 표현된 데이터에 대해서 성공적인 성과를 거두었지만, 최근에 non-Euclidean space에서 생성된 데이터에 딥러닝을 적용하려는 시도가 많아지고 있습니다. 본 논문에서는 GNN을 4개의 카테고리 (Recurrent GNN, Convolutional GNN, Graph autoencoder, Spatial-temporal GNN)으로 나누어서 소개하고 있습니다.2. Categorization and Frameworks먼저 GNN의 taxonomy와 framework에 대해서 간단하게 소개하고, 각각의 요소들을 나중에 자세히 다루겠습니다.2.1. TaxonomyRecurrent graph neural networks (RecGNNs)의 목표는 recurrent neural architecture를 사용해서 node representation을 학습하는 것입니다. RecGNNs에서는 각 노드가 안정상태가 될때까지 주위에 이웃 노드와 정보를 계속 교한한다는 가정을 합니다. RecGNNs는 개념적으로 중요하고 이후에 많은 연구에 영향을 주었는데, 특히 메시지 패싱 아이디어는 spatial-based convolutional GNN에서 사용됩니다.Convolutional graph neural networks (ConvGNNs)는 convolution 연산을 grid 에서 graph data로 확장했습니다. RecGNNs와 차이점은 ConvGNNs는graph convolutional layer를 여러개 쌓아서 고차원 노드 representation을 추출한다는 점입니다.Graph autoencoders (GAEs)는 node 혹은 graph를 latent space로 인코딩하는 프레임워크입니다. GAEs는 네트워크 임베딩이나 graph generative distribution을 만들 때 사용합니다.Spatial-temporal graph neural networks (STGNNs)는 spatial-temporal graph에서 숨겨진 정보를 학습하는 걸 목표로 합니다. STGNNs의 주요 아이디어는 spatial dependency와 temporal dependency를 동시에 고려한다는 점입니다. 현재 대부분 접근 방법들은 graph convolution을 통해 spatial 정보를 반영하고, RNN이나 CNN을 통해 temporal 정보를 반영하는 방식입니다.2.2. FrameworksGraph 구조와 노드에 포함된 정보가 입력으로 주어졌을 때, GNN의 출력값은 graph analytic task에 따라 달라집니다.Node-level의 출력값은 node regression이나 node classification과 관련이 있습다. RecGNN와 ConvGNN이 information propagation 혹은 graph convolution을 사용해서 고차원 node representation을 추출한 후에, multi-perceptron 혹은 softmax layer를 사용해서 node-level task를 수행합니다.Edge-level의 출력값은 edge classification 혹은 link prediction task와 관련이 있습니다. 두 개의 노드의 hidden representation을 입력으로 받고, 유사도 함수나 뉴럴 네트워크를 통해 edge의 레이블이나 연결 강도를 예측하는 테스크를 해결합니다.Graph-level의 출력값은 graph classification task와 연관이 있습니다. Graph 단위로 compact representation을 얻기 위해서 GNN은 종종 pooling 혹은 readout 연산과 결합이 됩니다.2.3. Training Frameworks많은 GNN은 (semi-) supervised 혹은 unsupervised 방식으로 학습이 가능합니다.  Semi-supervised learning for node-level classification일부 노드가 label 되고 나머지 노드가 label 되지 않았을 때, ConvGNN은 학습을 통해 unlabeled node에 대해서 label을 할당할 수 있습니다.  Supervised learning for graph-level classification그래프 수준의 분류는 전체 그래프에 대해서 class label을 예측하는 것을 목표로 합니다. 이 태스크에서 모델의 구조는 graph convolutional layer, graph pooling layer, readout layer로 이루어집니다. Graph convolutional layer에서는 고차원 node representation을 추출하고, graph pooling layer에서는 down sample을 수행해서 graph 구조를 coarse하게 변형합니다. 이후에 readout layer에서 node representation을 합쳐서 graph representation으로 만들고, multi-layer perceptron 혹은 softmax layer를 사용해서 그래프 레이블을 예측합니다.  Unsupervised learning for graph embeddingGraph에서 이용가능한 class label이 없을 때, unsupervised 방식으로 graph embedding을 할 수 있습니다. 두 가지 방식으로 수행이 가능한데, 첫째는 graph convolutional layer를 사용해서 graph를 인코딩하고, 디코더를 사용해서 graph를 복원하는 방법입니다. 두번째 방법은 negative sampling을 사용해서 negative node pair를 생성하고, 그래프에 있는 node pair를 positive로 삼는 방법입니다. 후에 regression layer를 사용해서 positive와 negative pair를 구분합니다. 이제 하나씩 자세히 살펴보겠습니다.3. Recurrent Graph Neural NetworksRecGNN은 GNN의 선구자입니다. 같은 파라미터 집합을 그래프의 전체 노드에 반복적으로 적용해서 고차원 노드 정보를 추출합니다. 계산량이 많아서 초기 연구들은 directed acyclic graphs (DAG)에만 적용이 되었습니다.이전에 DAG에 대해서만 다룬 GNN을 Scarselli가 acyclic, cyclic, directed, undirected graph로 확장해서 적용했습니다. Information diffusion mechanism을 기반으로, GNN은 안정 상태에 도달하기 전까지 이웃 노드와 정보를 교환하면서 노드의 정보를 다음과 같이 업데이트합니다.\\[\\mathbf{h}_v^{(t)}=\\sum_{u \\in N(v)} f\\left(\\mathbf{x}_v, \\mathbf{x}_{(v, u)}^{\\mathbf{e}}, \\mathbf{x}_u, \\mathbf{h}_u^{(t-1)}\\right)\\]현재 시점 노드 ($v$)와 이웃 노드 ($u$), edge, 이전시점 (t-1)의 이웃 노드 hidden state를 사용해서 현재 시점의 노드의 hidden state를 업데이트합니다. $\\mathbf{h}_v^{(0)}$ 는 처음에 임의로 초기화되고 $f$는 parametric 함수입니다. 수렴성을 보장하기 위해서, $f$는 반드시 contraction mapping 이어야 합니다. contraction mapping이란 latent space로 projection한 후에 두 점 사이의 거리가 줄어든다는 것을 의미합니다. $f$가 만약 뉴럴네트워크라면 Jacobian matrix를 통해 파라미터에 penalty term을 추가합니다. GNN이 반복적으로 node state를 전파하고 loss function에 대한 파라미터 gradient를 계산하다가, 수렴 조건이 만족되면 마지막 hidden state가 readout layer를 통과합니다.Gated Graph Neural Network (GGNN)은 gated recurrent unit (GRU)를 recurrent function으로 이용해서 반복횟수를 줄였습니다. 이 방법의 장점은 수렴성을 보장하기 위해 파라미터에 제약을 걸 필요가 없다는 점입니다. 업데이트는 다음과 같이 진행됩니다.\\[\\mathbf{h}_v^{(t)}=G R U\\left(\\mathbf{h}_v^{(t-1)}, \\sum_{u \\in N(v)} \\mathbf{W h}_u^{(t-1)}\\right)\\]이전 시점 (t-1)의 현재 노드 ($v$) 정보와 이웃 노드 (\\(u\\)) 정보를 GRU에 넣어서 현재 시점 노드를 업데이트 하는 방식입니다. 앞선 GNN과 차이점은 back-propagation through time (BPTT)를 사용한다는 점입니다. BPTT는 커다란 graph에 적용할 때 문제가 될 수 있는데, 왜냐하면 모든 노드의 intermediate state를 메모리에 저장해야하기 때문입니다.Stochastic Steady-state Embedding (SSE)는 더 큰 그래프에 적용가능한 scalable 학습 알고리즘을 제시합니다. SSE는 노드의 hidden state를 stochastic, asynchronous 방식으로 업데이트 합니다. 이 알고리즘은 반복적으로 노드 배치를 sampling하고, 해당 배치에 대해서 gradient를 계산합니다. 안정성을 보장하기 위해 SSE의 recurrent function은 이전 state와 현재 state의 weighted average로 정의됩니다.\\[\\mathbf{h}_v^{(t)}=(1-\\alpha) \\mathbf{h}_v^{(t-1)}+\\alpha \\mathbf{W}_{\\mathbf{1}} \\sigma\\left(\\mathbf{W}_{\\mathbf{2}}\\left[\\mathbf{x}_v, \\sum_{u \\in N(v)}\\left[\\mathbf{h}_u^{(t-1)}, \\mathbf{x}_u\\right]\\right]\\right)\\]이전 시점의 현재 노드 ($v$)의 hidden state와 이전 시점의 이웃 노드 ($u$)와 현재 노드 정보($\\mathbf{x}_v$)를 weighted average하는 방식입니다. 개념적으로는 중요하지만, 이론적으로 SSE의 수렴성은 증명되지 않았습니다.4. Convolutional Graph Neural NetworksConvGNN은 RecGNN과 관련이 깊습니다. 하나의 contraction 함수를 여러 layer에 반복적으로 적용하는 대신에, ConvGNN에서는 각 layer마다 다른 파라미터를 사용합니다.ConvGNN의 장점은 graph convolution 연산이 다른 뉴럴네트워크를 조합해서 사용하는 것보다 효율적이고 편리하기 때문입니다. 이로 인해 ConvGNN에 대한 관심이 최근에 증가했습니다.ConvGNN은 spatial 기반과 spectral 기반으로 나누어집니다. Spectral 기반 방법은 filter를 사용해서 graph convolution을 정의하고, graph convolution을 graph에서 noise를 제거하는 수단으로 해석합니다. Spatial 기반 방법은 RecGNN 영향을 받아서 graph convolution이 information propagation 역할을 한다고 간주합니다. GCN이 spectral 기반과 spatial 기반의 차이를 연결해준 이후로, spatial 기반 방법이 효율성, 유연성, 일반성 때문에 빠르게 발전했습니다.4.1. Spectral-based ConvGNNsSpectral 기반 방법은 graph signal processing에 있는 견고한 수학적 기반에서 만들어졌습니다. 여기서는 그래프가 undirected라 가정하고, normalized graph Laplacian matrix를 \\(\\mathbf{L}=\\mathbf{I}_{\\mathbf{n}}-\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\)으로 정의합니다. 여기서 \\(\\mathbf{D}\\)는 \\(\\mathbf{D}_{i i}=\\sum_j(\\mathbf{A}_{i, j})\\) 으로 정의된 대각행렬입니다. 그러면 \\(\\mathbf{L}\\)은 real symmetric positive semi-definite 성질을 갖고 있기 때문에, \\(\\mathbf{L}=\\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^T\\) 형태로 eigen decomposition할 수 있습니다.Graph signal processing에서, graph signal \\(\\mathbf{x}\\in \\mathbf{R}^n\\)은 각 노드의 값들을 모아서 만든 feature vector입니다 (\\(x_i\\)는 \\(i\\)번째 node의 값). Signal \\(\\mathbf{x}\\)에 대해서 graph Fourier 변환을 \\(\\mathscr{F}(\\mathbf{x})=\\mathbf{U}^T \\mathbf{x}\\)으로, inverse graph Fourier 변환을 \\(\\mathscr{F}^{-1}(\\hat{\\mathbf{x}})=\\mathbf{U} \\hat{\\mathbf{x}}\\)으로 정의하는데, 이는 graph signal을 normalized graph Laplacian 행렬의 eigenvector로 projection한 것 입니다. 이를 기반으로 입력 신호 \\(\\mathbf{x}\\)와 filter \\(\\mathbf{g}\\)에 대한 graph convolution을 다음과 같이 정의합니다.\\[\\begin{aligned}\\mathbf{x} *_G \\mathbf{g} &amp; =\\mathscr{F}^{-1}(\\mathscr{F}(\\mathbf{x}) \\odot \\mathscr{F}(\\mathbf{g})) \\\\&amp; =\\mathbf{U}\\left(\\mathbf{U}^T \\mathbf{x} \\odot \\mathbf{U}^T \\mathbf{g}\\right)\\end{aligned}\\]만약 \\(\\mathbf{g}_\\theta=\\operatorname{diag}\\left(\\mathbf{U}^T \\mathbf{g}\\right)\\)라고 한다면 위의 식을 다음과 같이 바꿀 수 있습니다.\\[\\mathbf{x} *_G \\mathbf{g}_\\theta=\\mathbf{U g}_\\theta \\mathbf{U}^T \\mathbf{x}\\]Spectral 기반 ConvGNN은 모두 위의 정의를 따르고, \\(\\mathbf{g}_{\\theta}\\)를 어떻게 선택하는지에 따라 차이가 있습니다.Spectral Convolutional Neural Network (Spectral CNN)에서는 filter를 \\(\\mathbf{g}_\\theta=\\boldsymbol{\\Theta}_{i, j}^{(k)}\\) 으로 두고, graph convolutional layer를 다음과 같이 나타냅니다.\\[\\mathbf{H}_{:, j}^{(k)}=\\sigma\\left(\\sum_{i=1}^{f_{k-1}} \\mathbf{U} \\boldsymbol{\\Theta}_{i, j}^{(k)} \\mathbf{U}^T \\mathbf{H}_{:, i}^{(k-1)}\\right) \\quad\\left(j=1,2, \\cdots, f_k\\right)\\]여기서 k는 layer index 이고, $\\mathbf{H}^{(k-1)} \\in \\mathbf{R}^{n \\times f_{k-1}}$ 는 이전 시점의 그래프 신호이고, $f_{k-1}$은 입력 채널의 개수, $f_k$는 출력채널의 개수, $\\Theta_{i, j}^{(k)}$는 학습가능한 파라미터로 이루어진 대각행렬입니다.위의 식을 해석하면, 각 출력채널 ($j$) 마다 이전시점의 모든 입력 채널 ($i)$을 $j$번째 filter를 이용해서 graph convolution을 적용한다는 의미입니다. CNN에서 각 filter마다 이미지의 다른 특성을 추출하듯이, 여기서도 filter마다 그래프의 다른 특성을 추출하게 됩니다.이 방법을 사용하려면 Laplacian matrix에 Eigen decomposition을 적용해야 하는데, 이로 인해서 3가지 제약사항이 있습니다. 첫째로 graph에 perturbation을 주면 eigen basis에 영향을 준다는 점이고, 두번째는 학습된 필터는 도메인에 따라 다르다는 점입니다. 이것의 의미는 어떤 구조에 학습된 필터는 다른 구조의 graph에 적용할 수 없다는 것입니다. 세번째는 eigen-decomposition은 $O\\left(n^3\\right)$만큼 계산량이 필요하다는 점입니다. 이후에 ChebNet과 GCN에서는 몇가지 근사방법을 통해 계산복잡도를 $O\\left(n\\right)$으로 줄입니다.Chebyshev Spectral CNN (ChebNet)에서는 filter $\\mathbf{g}_\\theta$를 Chebyshev polynomial을 사용해서 근사합니다.\\[\\mathbf{g}_\\theta=\\sum_{i=0}^K \\theta_i T_i(\\tilde{\\boldsymbol{\\Lambda}}), \\text { where } \\tilde{\\boldsymbol{\\Lambda}}=2 \\boldsymbol{\\Lambda} / \\lambda_{\\max }- \\boldsymbol{I_n}\\]$\\tilde{\\Lambda}$의 값은 [-1, 1] 사이에 있고, Chebyshev polynomial은 다음과 같습니다.\\[T_i(\\mathbf{x})=2 \\mathbf{x} T_{i-1}(\\mathbf{x})-T_{i-2}(\\mathbf{x}), \\;\\text{where} \\; T_0(\\mathbf{x})=1, T_1(\\mathbf{x})=\\mathbf{x}\\]이를 이용해서 graph convolution을 표현하면 다음과 같습니다.\\[\\mathbf{x} *_G \\mathbf{g}_\\theta=\\mathbf{U}\\left(\\sum_{i=0}^K \\theta_i T_i(\\tilde{\\mathbf{\\Lambda}})\\right) \\mathbf{U}^T \\mathbf{x}\\]\\[=\\sum_{i=0}^K \\theta_i T_i(\\tilde{\\mathbf{L}}) \\mathbf{x}\\]\\[\\text{where}\\;\\;T_i(\\tilde{\\mathbf{L}})=\\mathbf{U} T_i(\\tilde{\\mathbf{\\Lambda}}) \\mathbf{U}^T,  \\;\\;\\,\\,\\tilde{\\mathbf{L}}=2 \\mathbf{L} / \\lambda_{\\max }-\\mathbf{I}_{\\mathbf{n}}\\]Spectral CNN과 비교해서 개선한 점은 ChebNet에서 정의된 filter는 locality 특성을 갖고있습니다. 즉 graph size와 상관없이 local feature를 추출할 수 있습니다. (보충)Graph Convolutional Network (GCN)은 ChebNet의 first-order approximation입니다 $(K=1, \\lambda_{max}=2)$. 그러면 ChebNet의 graph convolution을 다음과 같이 간소화할 수 있습니다.\\[\\mathbf{x} *_G \\mathbf{g}_\\theta=\\theta_0 \\mathbf{x}-\\theta_1 \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{x}\\]여기서 파라미터 개수를 줄이고 오버피팅을 예방하기 위해서 $\\theta=\\theta_0=-\\theta_1$ 라는 가정을 해서 다음과 같이 위의식을 바꿉니다.\\[\\mathbf{x} *_G \\mathbf{g}_\\theta=\\theta\\left(\\mathbf{I}_{\\mathbf{n}}+\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\right) \\mathbf{x}\\]Multi-channel 입력과 출력을 허용하기 위해서, GCN은 compositional layer 형태로 표현이 가능합니다.\\[\\mathbf{H}=\\mathbf{X} *_G \\mathbf{g}_{\\boldsymbol{\\Theta}}=f(\\overline{\\mathbf{A}} \\mathbf{X} \\boldsymbol{\\Theta})\\]$\\mathbf{I}_{\\mathbf{n}}+\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A D}^{-\\frac{1}{2}}$값은 numerical instability를 초래한다는 실험적인 결과가 있어서, 이를 완화하기 위해 다음과 같은 normalization trick을 사용합니다.\\[\\overline{\\mathbf{A}}=\\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-\\frac{1}{2}} \\text { with } \\tilde{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}_{\\mathbf{n}} \\text { and } \\tilde{\\mathbf{D}}_{i i}=\\sum_j \\tilde{\\mathbf{A}}_{i j}\\]GCN은 또한 spatial-based 방법으로도 해석이 가능합니다. Spatial 관점에서 GCN은 이웃 노드의 feature information을 종합한 것으로 해석이 가능합니다.\\[\\mathbf{h}_v=f\\left(\\boldsymbol{\\Theta}^T\\left(\\sum_{u \\in\\{N(v) \\cup v\\}} \\bar{A}_{v, u} \\mathbf{x}_u\\right)\\right) \\quad \\forall v \\in V\\]GCN을 점진적으로 개선한 방법들이 최근에 등장하고 있습니다. Adaptive Graph Convolutional Network (AGCN)에서는 관계를 adjacency matrix로 표현하지 않고 hidden structural relation을 학습합니다. 두 노드 feature를 입력으로 받아서 학습가능한 distance function을 통해 residual graph adjacency matrix라고 부르는 adjacency matrix를 만듭니다.Dual Graph Convolutional Network (DGCN)에서는 두 개의 graph convolutional layer를 병렬적으로 합친 graph convolutional architecture를 제안했습니다. 두 레이어는 파라미터를 공유하고, normalized adjacency matrix $\\overline{\\mathbf{A}}$와 positive pointwise mutual information (PPMI) matrix를 사용합니다. PPMI는 graph에서 샘플링된 random walk를 통해 노드의 co-occurrence 정보를 추출합니다. PPMI 행렬은 다음과 같이 정의됩니다.\\[\\mathbf{P P M I}_{v_1, v_2}=\\max \\left(\\log \\left(\\frac{\\operatorname{count}\\left(v_1, v_2\\right) \\cdot|D|}{\\operatorname{count}\\left(v_1\\right) \\operatorname{count}\\left(v_2\\right)}\\right), 0\\right)\\]\\[\\text { where } v_1, v_2 \\in V,|D|=\\sum_{v_1, v_2} \\operatorname{count}\\left(v_1, v_2\\right)\\]$\\operatorname{count}(\\cdot)$함수는 random walk에서 node $v$와 node $u$가 동시로 등장하는 빈도를 반환합니다. (PPMI 해석 추가) Dual graph convolutional layer에서 얻은 output을 앙상블해서 DGCN은 여러 graph convolutional layer를 사용하지않고, local하고 global한 구조적인 정보를 인코딩합니다.4.2. Spatial-based ConvGNNs이미지에서 사용하는 전통적인 convolution 연산처럼, spatial 기반 방법은 노드의 spatial 관계를 기반으로 graph convolution을 정의합니다. 이 아이디어는 RecGNN에서 사용한 message passing과 유사하다.Neural Network for Graphs (NN4G) 는 ConvGNN을 spatial 기반으로 설계한 첫번째 모델입니다. NN4G는 각 layer의 독립적인 파라미터를 두어서 graph dependency를 학습합니다. 여러 layer를 쌓으면서 노드의 이웃 범위가 점점 넓어지고, graph convolution을 통해 모든 이웃을 합친 정보를 생성합니다. 또한 residual connection을 사용해서 각 layer의 정보를 보존합니다.Diffusion Convolutional Neural Network (DCNN) 은 graph convolution을 diffusion process로 간주합니다. 정보가 한 노드에서 다른 노드로 퍼질 때 특정한 확률을 갖고 몇번 전파된 이후에 information distribution이 평형상태에 도달한다고 가정합니다. DCNN은 diffusion graph convolution을 다음과 같이 정의합니다.\\[\\mathbf{H}^{(k)}=f\\left(\\mathbf{W}^{(k)} \\odot \\mathbf{P}^k \\mathbf{X}\\right)\\]여기서 $f$는 activation function이고, $\\mathbf{P}$는 probability transition matrix이고, $\\mathbf{P}=\\mathbf{D}^{-1} \\mathbf{A}$ 형태로 계산이 가능합니다. DCNN은 $\\mathbf{H}^{(1)}, \\mathbf{H}^{(2)}, \\cdots, \\mathbf{H}^{(K)}$를 모두 concatenate한 값을 final model output으로 설정합니다. Diffusion process의 stationary distribution은 $\\mathbf{P}$의 power series의 합이기 때문에, Diffusion Graph Convolution (DGC)는 다음과 같이 각 diffusion step마다 output을 합친 것입니다.\\[\\mathbf{H}=\\sum_{k=0}^K f\\left(\\mathbf{P}^k \\mathbf{X} \\mathbf{W}^{(k)}\\right)\\]여기서 $K$는 diffusion step인데 높아질수록 넓은 범위까지 반영한다는 것입니다. $k$값이 커질수록 transition matrix가 여러번 곱해져서 확률값이 작아지기 때문에, 멀리있는 이웃노드들은 반영되는 비율이 적어집니다.Partition Graph Convolution (PGC)는 현재 노드 이웃을 특정 기준에따라 Q개의 group으로 나누어서 Q개의 adjacency matrix를 생성합니다. 그래서 각 이웃노드 그룹마다 다른 파라미터를 사용해서 GCN을 다음과같이 적용합니다.\\[\\mathbf{H}^{(k)}=\\sum_{j=1}^Q \\overline{\\mathbf{A}}^{(j)} \\mathbf{H}^{(k-1)} \\mathbf{W}^{(j, k)}\\]\\[\\;\\;\\;\\;\\,\\,\\,\\,\\,\\,\\,\\text { where } \\mathbf{H}^{(0)}=\\mathbf{X}, \\overline{\\mathbf{A}}^{(j)}=\\left(\\tilde{\\mathbf{D}}^{(j)}\\right)^{-\\frac{1}{2}}$ \\tilde{\\mathbf{A}}^{(j)}\\left(\\tilde{\\mathbf{D}}^{(j)}\\right)^{-\\frac{1}{2}}\\]\\[\\tilde{\\mathbf{A}}^{(j)}=\\mathbf{A}^{(j)}+\\mathbf{I}\\]Q개의 group마다 Graph convolution을 적용하고 summation을 통해 최종적으로 현재 노드의 hidden state를 업데이트 합니다.Message Passing Neural Network (MPNN)은 spatial 기반 ConvGNN의 일반적인 framework입니다. Graph Convolution을 message passing으로 간주해서, 정보가 한 노드에서 다른 노드로 직접 전달됩니다. Message passing 함수는 다음과 같이 정의됩니다.\\[\\mathbf{h}_v^{(k)}=U_k\\left(\\mathbf{h}_v^{(k-1)}, \\sum_{u \\in N(v)} M_k\\left(\\mathbf{h}_v^{(k-1)}, \\mathbf{h}_u^{(k-1)}, \\mathbf{x}_{v u}^e\\right)\\right)\\]\\[\\text { where } \\mathbf{h}_v^{(0)}=\\mathbf{x}_v\\]\\(U_k(\\cdot)\\)와 \\(M_k(\\cdot)\\)는 학습가능한 파라미터로 이루어진 함수입니다. 이전시점의 현재노드 hidden state \\(\\mathbf{h}_v^{(k-1)}\\) 이랑 이웃노드 \\(\\mathbf{h}_u^{(k-1)}\\), 엣지정보  \\(\\mathbf{x}_{v u}^e\\)를 입력으로 받아서 변환하고 합친정보를 다시 $\\mathbf{h}_v^{(k-1)}$와 합쳐서 $U_k$를 통해 변환하는 방식으로 업데이트를 합니다.Graph Isomorphism Network (GIN)에서는 MPNN의 한계점을 발견했는데, MPNN 기반 방법에서그래프 임베딩을 통해 graph 구조를 구분하는게 불가능 하다는 것입니다. 이런 단점을 보완하기 위해 GIN에서는 중심노드의 가중치에 학습파라미터를 추가해서 graph convolution을 다음과 같이 수행합니다.\\[\\mathbf{h}_v^{(k)}=M L P\\left(\\left(1+\\epsilon^{(k)}\\right) \\mathbf{h}_v^{(k-1)}+\\sum_{u \\in N(v)} \\mathbf{h}_u^{(k-1)}\\right)\\]가중치 파라미터를 통해 중심노드와 주변노드를 구분해서 현재 노드를 업데이트를 합니다.노드 이웃의 개수는 1개일 수도 있고 천개가 넘어갈 수도 있기 때문에, 모든 노드의 정보를 반영하는 것은 비효율 적일 가능성이 있습니다. 그래서 GraphSage에서는 각 노드에 반영될 이웃 노드의 개수를 고정시키고 sampling을 통해 convolution을 수행합니다.\\[\\mathbf{h}_v^{(k)}=\\sigma\\left(\\mathbf{W}^{(k)} \\cdot f_k\\left(\\mathbf{h}_v^{(k-1)},\\left\\{\\mathbf{h}_u^{(k-1)}, \\forall u \\in S_{\\mathcal{N}(v)}\\right\\}\\right)\\right)\\]$S_{\\mathcal{N}(v)}$는 노드 $v$의 이웃 노드의 랜덤 샘플입니다. 몇 개의 이웃노드를 랜덤으로 선택해서 이웃노드의 hidden state와 이전시점의 현재노드 hidden state $\\mathbf{h}_v^{(k-1)}$를 합쳐서 convolution 연산을 수행합니다. Aggregation 함수 $f$는 mean, sum, max 함수처럼 노드 순서에 invariant한 성질을 가진 함수를 사용해야합니다.Graph Attention Network (GAT)는 주변 노드들이 중심노드에 업데이트하는 비율이 동일하지 않고,미리 정해져 있지도 않습니다.GCN은 업데이트 비율이 미리 정해져있고 GraphSage에서는 주변노드 반영비율이 모두 동일한데, GAT에서는 attention mechanism을 통해 각 노드의 반영비율이 유동적으로 모두 다르게 설정합니다.\\[\\mathbf{h}_v^{(k)}=\\sigma\\left(\\sum_{u \\in \\mathcal{N}(v) \\cup v} \\alpha_{v u}^{(k)} \\mathbf{W}^{(k)} \\mathbf{h}_u^{(k-1)}\\right)\\]$\\alpha_{v u}^{(k)}$값은 node $v$와 $u$의 연결 강도를 측정합니다. 추가적으로 GAT는 multi-head attention을 통해 모델의 표현력을 높였는데, GraphSage보다 node classification task에서 성능이 많이 좋아졌습니다.GAT에서는 각 attention head마다 같은 분포를 갖는다고 가정하는데, Gated Attention Network (GANN)에서는 각 attention head마다 attention score를 따로 계산합니다. 다른 종류의 graph attention 모델도 있지만, 그들은 ConvGNN framework에 속하지 않아서 여기서 소개하지는 않습니다.4.3. Improvement in terms of training efficiencyGCN같은 ConvGNN을 학습하려면 전체 graph data와 모든 노드의 중간 hidden state를 저장해야돼서 메모리가 많이 필요합니다. 메모리를 절약하기 위해 GraphSage는 batch-training algorithm을 제안했는데,Fast Learning with Graph Convolutional Network (FastGCN)에서는 각 layer마다 고정된 수의 노드를 샘플링합니다. 그래서 각 layer마다 연결이 sparse하게 돼서 학습속도가 빨라집니다. Huang은 adaptive layer-wise sampling 기법을 제시하는데, top layer에 노드가 샘플링 되었을 때, 이를 기반으로 bottom layer 노드를 샘플링합니다. 더 복잡한 샘플링 기법을 사용해서 FastGCN보다 정확도를 높였습니다.[Stochastic Training of Graph Convolutional Networks] (StoGCN)](https://arxiv.org/pdf/1710.10568.pdf)은 graph convolution의 receptive field 크기를 줄였습니다. Historic node representation을 통제변수로 사용했습니다. 노드 당 두개의 이웃노드만 사용해도 상당한 성능을 달성했습니다. 하지만 여전히 모든 노드의 intermediate state를 저장해야하기 떄문에 메모리 소비가 GCN과 같습니다.Cluster-GCN 에서는 그래프 클러스터링 알고리즘을 사용해서 sub graph를 추출하고, sub-graph에 graph convolution을 적용합니다. 이웃노드 탐색 또한 sub-graph로 제한이 되어서 더 짧은 시간에 더 적은 메모리로 graph convolution 연산이 가능합니다.s: 배치사이즈 / K: layer 수 /r: 각 노드마다 샘플된 이웃 노드의 수 / d: feature dimension위의 표를 보면 GraphSage는 시간이 더 소모되지만 메모리가 절약됩니다. StoGCN의 시간복잡도는 가장 크고, 메모리 소비량도 GCN과 비슷합니다. 하지만 StoGCN은 r값이 작아도 좋은 성능을 유지할 수 있습니다. Cluster-GCN은 time complexity는 동일한데 메모리 소비량은 적습니다.4.4. Comparison between spectral and spatial modelsSpectral 모델은 graph signal processing의 이론적인 기반위에 세워졌습니다. 새로운 graph signal filter를 사용해서 새로운 ConvGNN을 만들 수 있지만, 일반적으로 spatial model이 효율성과 유연성 때문에 더 선호가 됩니다.Spectral 기반 방법은 eigenvector를 계산해야하거나 전체 그래프를 한번에 다루어야 하는 경우가 많습니다. Spatial 모델은 information propagation을 통해 convolution을 수행하기 때문에, large graph에서도 적용이 가능합니다. 계산도 전체 그래프에 수행하는 대신에 일부 노드 배치에 대해서만 수행합니다.또한 spectral 모델은 graph Fourier basis에 기반하기 때문에 새로운 그래프로 일반화가 되기 어렵습니다. Spectral 모델은 고정된 그래프를 가정하는데, 왜냐하면 Graph에 perturbation을 주면 eigen basis가 변하기 때문입니다. 반면 Spatial 기반 모델은 각 노드에 지역적으로 graph convolution 연산을 수행하기 때문에, weight이 다른 지역이나 구조로 공유될 수 있습니다.마지막으로 spectral 기반 방법은 undirected graph로 한정이 되는데, spatial 기반 방법은 더 유연해서 다양한 graph(edge, directed graph, signed graph, hetereogeneous graph)를 입력으로 받을 수 있습니다.                   Spectral      Spatial                  Scalability      Less (Due to eigenvector computation)      High              Generalization      Poor (Fixed graph assumption)      High              Available graph      Undirected      Undirected , Directed, edge, signed, heterogeneous      5. Graph Pooling ModulesGNN이 node feature를 생성하면 task에 따라 다르게 사용이 가능합니다. 모든 feature를 직접적으로 사용하는 건 계산량이 많아서 down-sampling을 사용합니다. 목적에 따라 다른 이름으로 사용이 되는데, 파라미터 크기를 줄이기 위해 down sampling하는 것은 pooling operation이라고 부르고, graph-level representation을 추출할 때 사용하면 readout operation이라고 부릅니다. 두 개의 mechanism은 유사합니다. 이번 챕터에서는 pooling을 down-sampling의 의미로 사용하겠습니다.일반적으로 mean/max/sum pooling이 가장 기초적이고 효과적인 down sampling 방법입니다. DCGNN에서는 SortPooling이라는 pooling 방법을 제시했는데, 이 방법은 노드의 순서를 그래프 구조에서 노드의 역할에 따라 의미적으로 배열해서 풀링을 수행합니다.Differentiable pooling (DiffPool)에서는 그래프의 계층적인 표현을 생성합니다. 이전의 corasening 기반 방법과는 다르게 DiffPool은 각 layer $k$ 마다 cluster assignment matrix $S$를 다음과 같이 학습가능하게 만듭니다.\\[\\mathbf{S}^{(k)}=\\operatorname{softmax}\\left(\\operatorname{Conv} G N N_k\\left(\\mathbf{A}^{(k)}, \\mathbf{H}^{(k)}\\right)\\right)\\]이런 방법의 중심이 되는 생각은 그래프의 위상적인 정보와 feature 정보를 동시에 고려해서 node assignment를 수행한다는 점입니다. 하지만 DiffPool의 단점은 풀링 이후에 dense graph를 생성해서 계산복잡도가 $O\\left(n^2\\right)$이 됩니다.SAGPool에서는 node feature와 graph topology를 모두 고려하면서 self-attention 방식응로 풀링을 수행합니다.풀링 연산은 그래프 크기를 줄일 수 있는 효율적인 연산입니다. 풀링을 통해 계산복잡도를 어떻게 낮출 수 있는지는 여전히 열러있는 문제입니다.6. Discussion of Theoretical Aspects여기서는 GNN의 이론적인 기반에 대해 살펴보겠습니다.6.1. Shape of receptive field노드의 receptive field는 마지막 노드 representation에 기여한 노드의 집합입니다. 여러 spatial graph convolutional layer를 사용하면 노드의 receptive field는 커집니다. Micheli는 고정된 크기의 spatial graph convolutional layer를 사용해서 모든 노드를 커버하는 receptive field를 만들 수 있다고 증명했습니다. 그 결과, ConvGNN은 몇개의 local graph convolutional layer를 사용해서 global information을 추출할 수 있습니다.6.2. VC dimensionVC dimension은 모델에의해 분해될 수 있는 최대 점의 수로 정의되는, 모델 복잡도를 측정하는 방법입니다. GNN의 VC dimension을 분석하는 몇가지 논문이 있었습니다. 모델 파라미터 개수 p와 노드 수 n이 주어졌을 때, Scarselli는 GNN에서 tangent hyperbolic 혹은 sigmoid activation을 사용했을 경우에 VC dimension이 $O\\left(p^4 n^2\\right)$라고 주장합니다. 이 결과는 GNN의 모델 복잡도가 p와 n에 따라 빠르게 증가한다는 것을 암시합니다.6.3. Graph isomorphism두 그래프는 위상적으로 동일하면 isomorphic합니다. non-isomorphic graph $G_1, G_2$가 있을 때, Xu는 GNN이 $G_1$과 $G_2$를 다르게 임베딩한다면 Weisfeiler-Lehman (WL) test를 통해 두 그래프가 non-isomorphic하다고 밝혀낼 수 있다고 합니다. GCN이나 GraphSage같은 일반적인 GNN에서는 두 그래프 구조를 구분하는게 불가능하다고 말합니다. 만약 aggregation function과 readout function이 injective 하다면, GNN을 통해 두 그래프를 구분할 수 있다고 합니다.6.4. Equivariance and invarianceGNN은 node-level task를 수행할 떄는 equivariant해야하고, graph-level task를 수행할 때는 invariant해야 합니다.$Q$가 임의의 permutation matrix라 하고, $f(\\mathbf{A}, \\mathbf{X})$가 GNN이라고 하면$f\\left(\\mathbf{Q A Q}{ }^T, \\mathbf{Q X}\\right)=\\mathbf{Q} f(\\mathbf{A}, \\mathbf{X})$을 만족할 때 f는 equivariant하고, $f\\left(\\mathbf{Q A Q}^T, \\mathbf{Q X}\\right)=f(\\mathbf{A}, \\mathbf{X})$를 만족할 떄 f는 invariant 합니다. Equivariance와 invariance 성질을 갖기 위해 GNN component는 노드 순서에 영향을 받지 않아야 합니다.7. Graph Autoencoders (GAE)Graph Autoencoder는 노드를 feature space로 임베딩하고 feature space에서 graph 정보를 생성합니다. GAE는 네트워크 임베딩 혹은 새로운 그래프를 생성할 때 사용할 수 있습니다.7.1. Network Embedding네트워크 임베딩은 low-dimension에서 노드의 위상적인(topological) 정보를 보존한 채로 노드를 저차원 벡터로 표현한 것입니다. GAE는 인코더를 통해 네트워크 임베딩을 학습하고 디코더를 통해 PPMI와 인접행렬처럼 그래프의 위상적인 정보를 네트워크 임베딩에 반영합니다.초기 접근은 주로 multi-layer perceptron을 이용했습니다. SDNE에서는 인코더를 통해 노드의 first-order proximity(현재 노드와 직접적으로 연결된 노드 정보)와 second-order proximity(이웃 노드들의 이웃 노드 정보)를 반영했습니다.Second-order proximity를 좀 더 자세하게 설명하자면, 만약 어떤 노드 pair $(u, v)$가 있을 때 두 노드를 연결하는 edge의 weight $w_{u,v}$는 first-order proximity입니다.그러면 모든 이웃 노드를 반영해서 하나의 벡터 $p_u = (w_{u,1}, …, w_{u,|V|})$를 만들 수 있습니다. Second-order proximity는 $p_u$와 $p_v$ 사이에 유사도를 의미합니다. 이것의 의미를 해석하면, 공유하고 있는 이웃노드들이 많으면 두 노드는 유사하다고 판단하는 것입니다.네트워크 임베딩에 First-order proximity를 반영하기 위해서 인접한 두 노드의 임베딩이 같도록 다음과 같은 loss function을 사용했습니다.\\[L_{1 s t}=\\sum_{(v, u) \\in E} A_{v, u}\\left\\|e n c\\left(\\mathbf{x}_v\\right)-e n c\\left(\\mathbf{x}_u\\right)\\right\\|^2\\]위의 loss function은 이웃하는 노드들의 임베딩을 유사하게 만들어주는 역할을 하고, 네트워크의 local structure 정보를 임베딩에 반영하게 됩니다.그리고 second-order proximity를 반영하기 위해서 입력 노드와 생성된 노드가 같아지도록 다음과 같은 loss function을 사용했습니다.\\[L_{2 n d}=\\sum_{v \\in V}\\left\\|\\left(\\operatorname{dec}\\left(\\operatorname{enc}\\left(\\mathbf{x}_v\\right)\\right)-\\mathbf{x}_v\\right) \\odot \\mathbf{b}_v\\right\\|^2\\]\\[\\text { where } b_{v, u}=1 \\text { if } A_{v, u}=0, b_{v, u}=\\beta&gt;1 \\text { if } A_{v, u}=1\\]여기서 두 노드가 연결이 되어있으면 1보다 큰 가중치를 주고, 연결되어있지 않으면 1만큼 반영하는 식으로 가중치를 할당했습니다. Second-order proximity를 통해서는 네트워크의 global structure 정보를 임베딩에 반영하게 됩니다.SDNE는 노드 사이의 연결성에 관련된 정보만 고려를 했습니다. 노드 자체가 갖고있는 feature정보를 사용하지 않았는데, GAE*에서는 노드의 구조적인 정보와 노드 feature 정보를 모두 반영해서 인코딩하려고 GCN을 encoder로 사용하는 방법을 제시했습니다.\\[\\mathbf{Z}=e n c(\\mathbf{X}, \\mathbf{A})=G \\operatorname{conv}\\left(f\\left(G \\operatorname{conv}\\left(\\mathbf{A}, \\mathbf{X} ; \\boldsymbol{\\Theta}{\\mathbf{1}}\\right)\\right) ; \\boldsymbol{\\Theta}{\\mathbf{2}}\\right)\\]ㅇ여기서 \\(\\mathbf{Z}\\)는 네트워크 임베딩 행렬이고 \\(f\\)는 ReLU, \\(Gconv\\)는 graph convolutional layer 입니다. 이렇게 네트워크를 임베딩하고, decoder에서는 네트워크 임베딩에서 인접행렬 \\(A\\)를 복원하는 것을 목표로 합니다.\\[\\hat{\\mathbf{A}}_{v, u}=\\operatorname{dec}\\left(\\mathbf{z}_v, \\mathbf{z}_u\\right)=\\sigma\\left(\\mathbf{z}_v^T \\mathbf{z}_u\\right)\\]그래서 GAE*에서는 실제 인접행렬 \\(A\\)와 모델을 통해 다시 만들어진 행렬 \\(\\hat{A}\\)사이에 cross entropy를 낮추는 방향으로 학습이 진행이 됩니다.그런데 논문에서는 단순히 인접행렬을 복원하는 방향으로 학습을 하면 overfitting이 생길 수 있다고 생각해서, 해당 문제를 해결할 수 있는 Variational Graph Autoencoder (VGAE)를 제안합니다. VGAE에서는 다음과 같은 lower bound를 최대화 하는 방향으로 학습을 진행합니다.\\[\\begin{aligned}&amp; L=E_{q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})}[\\log p(\\mathbf{A} \\mid \\mathbf{Z})]-K L[q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A}) \\| p(\\mathbf{Z})] \\\\\\\\&amp; \\;\\;\\;\\;\\;\\;\\;\\; p\\left(A_{i j}=1 \\mid \\mathbf{z}_i, \\mathbf{z}_j\\right)=\\operatorname{dec}\\left(\\mathbf{z}_i, \\mathbf{z}_j\\right)=\\sigma\\left(\\mathbf{z}_i^T \\mathbf{z}_j\\right) \\\\&amp; \\;\\;\\;\\;\\;\\;\\;\\; q(\\mathbf{Z} \\mid \\mathbf{X}, \\mathbf{A})=\\prod_{i=1}^n q\\left(\\mathbf{z}_i \\mid \\mathbf{X}, \\mathbf{A}\\right) \\text { with } q\\left(\\mathbf{z}_i \\mid \\mathbf{X}, \\mathbf{A}\\right)=N\\left(\\mathbf{z}_i \\mid \\mu_i, \\operatorname{diag}\\left(\\sigma_i^2\\right)\\right)\\end{aligned}\\]여기서 \\(p(\\mathbf{Z})\\)는 factorized gaussian prior입니다. VGAE에서는 VAE처럼 인코더의 아웃풋을 정규분포 파라미터를 통해 추정합니다. 실제로 link prediction 실험 결과에서도 3개 dataset 중 2개 dataset (Cora, Citeseer)에서 VGAE가 GAE보다 성능이 좋았고, 노드 feature 정보를 사용하지 않았을 때 보다 사용했을때는 모든 데이터셋에 대해서 성능이 향상되었습니다.GAE*와 유사하게 GraphSage에서는 2개의 GCL을 통해 노드 피처를 인코딩합니다. GAE*와 차이점은 recon-struction error를 사용하지 않고, 두 노드의 관계정보를 negative sampling을 통해 임베딩에 반영합니다.\\[L\\left(\\mathbf{z}_v\\right)=-\\log \\left(\\operatorname{dec}\\left(\\mathbf{z}_v, \\mathbf{z}_u\\right)\\right)-Q E{v_n \\sim P_n(v)} \\log \\left(-\\operatorname{dec}\\left(\\mathbf{z}_v, \\mathbf{z}_{v_n}\\right)\\right)\\]\\(P_n(v)\\)는 negative sample distribution이고, Q는 negative sample 개수입니다. 위의 loss function은 이웃 노드들에 대해서는 유사한 embedding이 생성되는 방향으로, negative sample(이웃하지 않은 노드)들에 대해서는 유사하지 않는 embedding이 생성되는 방향으로 학습이 진행됩니다.지금까지 소개한 방법들은 link prediction 문제를 해결하면서 네트워크 임베딩을 학습했스비다. 하지만 sparse graph의 경우에는 positive node pair의 수가 negative node pair보다 현저하게 작아서 이웃 노드 임베딩이 유사해지지 않을 가능성이 생깁니다. 이 문제를 해결하기 위해서 그래프를 random permutation 혹은 random walk를 사용해서 sequence 형태로 변형하는 방향으로 진행되는 연구도 있습니다.Deep Recursive Network Embedding (DRNE)에서는 regular invariance라는 개념을 사용해서 네트워크 임베딩을 수행합니다. Regular invariance란 어떤 두 노드가 공통된 이웃 노드가 존재하지 않더라도, 비슷한 역할을 수행하는 노드입니다. 다시말해서, 네트워크 구조와 상관없이 그 자체로 노드가 유사하면 두 노드는 regular invariant하다고 정의합니다.Regular invariant 노드를 유사하게 임베딩하기 위해서 DRNE는 노드의 네트워크 임베딩이 이웃한 네트워크 임베딩의 aggregation을 근사한 것이다라고 가정을 합니다. 그리고 이웃노드를 degree별로 오름차순으로 정렬해서 LSTM을 사용해서 노드의 이웃을 다음과 같이 aggregate 합니다.\\[L=\\sum_{v \\in V}\\left\\|\\mathbf{z}_v-\\operatorname{LSTM}\\left(\\left\\{\\mathbf{z}_u \\mid u \\in N(v)\\right\\}\\right)\\right\\|^2\\]위의 식은 LSTM에서 임베딩을 생성하는 것이 아니라, 노드의 임베딩을 이웃노드의 임베딩을 aggregation해서 생성하고 있습니다. 조금 이해가 안가는 점은degree를 기준으로 노드 임베딩을 정렬해서 LSTM을 통해 aggregation 하는 것이 regular invariance정보와 어떤 관련이 있는지 잘 이해가 가지 않았습니다.7.2. Graph GenerationGAE는 학습 과정을 통해 graph에 대한 generative distribution을 학습할 수 있습니다. Graph generation을 위한 GAE는 대부분 molecular graph generation 문제를 해결하기 위해 제안되었습니다. Graph generation 접근방법에는 sequential 방식과 global 방식 두가지가 있습니다.Sequential 방식에는 Deep Generative Model of Graphs (DeepGMG)라는 방법이 있는데, 이 방법은 그래프의 확률을 모든 가능한 node permutation의 합으로 가정합니다.\\[p(G)=\\sum_\\pi p(G, \\pi)\\]여기서 \\(\\pi\\)는 노드 순서를 의미합니다. 이 확률은 그래프에서 모든 노드와 엣지의 join probability를 반영합니다. DeepGMG에서 그래프를 생성할 때 순차적으로 결정을 내리는데, 주로 노드 혹은 엣지를 추가할지 말지, 어떤 노드를 추가할지를 결정합니다. 노드 state 혹은 graph state에 따라 node나 edge를 생성하는 결정과정이 달라집니다.Global 방식에서는 그래프를 한번에 출력합니다. Graph Variational Autoencoder (GraphVAE)에서는 독립적인 확률변수로 노드와 엣지의 존재여부를 모델링 하고, 다음과 같은 lower bound를 최대화 합니다.\\[L(\\phi, \\theta ; G)=E_{q_\\phi(z \\mid G)}\\left[-\\log p_\\theta(G \\mid \\mathbf{z})\\right]+K L\\left[q_\\phi(\\mathbf{z} \\mid G) \\| p(\\mathbf{z})\\right]\\]ConvGNN을 인코더로 삼고, multi-layer perceptron을 decoder로 사용해서 인접 행렬과 함께 그래프를 생성합니다. 이런 방식은 생성된 그래프의 global property (graph connectivity, validity) 같은 요소를 통제하기가 어렵습니다.8. Spatial-Temporal Graph Neural Networks (STGNN)현실세계에서 그래프 구조나 그래프에 들어오는 input은 대부분 dynamic한 성질을 갖고있습니다. STGNN은 그래프의 dynamicity를 반영하는 방법입니다. STGNN은 그래프의 spatial, temporal dependency를 동시에 학습합니다. 주로 future node value를 예측하거나, 현재 그래프의 label을 예측하는 데 사용이 됩니다. 이 방법은 RNN 기반 방법과 CNN 기반 방법 두가지가 있습니다.대부분 RNN 기반 방법은 Graph convolution을 통해 현재 시점 입력과 이전 시점 hidden state를 다음 state로 넘기는 방식으로 다음과 같이 이루어져 있습니다.\\[\\mathbf{H}^{(t)}=\\sigma\\left(G \\operatorname{conv}\\left(\\mathbf{X}^{(t)}, \\mathbf{A} ; \\mathbf{W}\\right)+G \\operatorname{conv}\\left(\\mathbf{H}^{(t-1)}, \\mathbf{A} ; \\mathbf{U}\\right)+\\mathbf{b}\\right)\\]위의 식은 현재 시점 입력 $\\mathbf{X}^{(t)}$와 이전 시점 hidden state $\\mathbf{H}^{(t-1)}$이 graph convolution을 통해서 현재 시점의 hidden state로 변하는 과정입니다.RNN 기반인데 다른 방식의 연구는 node-level RNN과 edge-level RNN을 동시에 사용하는 방법입니다. Structural-RNN에서는 각 time step마다 node-RNN을 사용해서 노드의 temporal 정보를 반영하고, edge-RNN을 사용해서 edge의 temporal 정보를 반영합니다. 그리고 spatial 정보를 통합하기 위해서 edge-RNN의 출력값을 node-RNN의 입력으로 집어넣습니다. 이를 통해 각 time step마다 node label을 예측하는 모델을 제안했습니다.RNN 기반 방법들은 순차적으로 데이터가 모델에 전달이 되기 때문에 시간이 많이 소모되고, gradient explosion/vanishing 문제도 존재합니다. 그래서 CNN 기반 방법에서는 순차적으로 데이터를 처리하지 않고 병렬적으로 처리해서 spatial temporal graph를 학습합니다.위의 그림처럼 CNN 기반 방법은 Graph Convoluion을 통해 spatial 정보를 학습하고 1D-CNN을 통해 temporal 정보를 학습합니다. 1D-CNN은 각 노드마다 시간축을 따라 convolution을 진행해서 temporal 정보를 aggregation 합니다.지금까지 소개한 방법들은 그래프 구조가 미리 정해진 상태였습니다. 하지만  spatial-temporal setting에서는 그래프 데이터를 통해서 graph structure를 학습할 수 있습니다. Graph WaveNet에서는 graph convolution을 수행할 때 다음과 같이 정의된 self-adaptive adjacency matrix를 사용했습니다.\\[\\mathbf{A}_{a d p}=\\operatorname{SoftMax}\\left(\\operatorname{ReLU}\\left(\\mathbf{E}_1 \\mathbf{E}_2^T\\right)\\right)\\]\\(\\mathbf{E_1}\\) 은 source node embedding을 가리키고, \\(\\mathbf{E}_2\\)는 target node embedding을 가리킵니다. \\(\\mathbf{E}_1\\) 과 \\(\\mathbf{E}_2\\) 를 곱해서 source node와 target node 사이에 dependency weight을 얻을 수 있고, 이를 통해 인접 행렬을 추정하는 방식입니다. Graph WaveNet은 인접행렬이 주어지지 않아도 좋은 성능을 내는 결과를 보여주었습니다.GaAN에서는 attention mechanism을 통해 dynamic spatial dependency를 학습했습니다. Attention을 통해서 연결된 두 노드 사이에 edge weight를 업데이트를 했습니다. ASTGCN에서는 spatial 뿐만 아니라 temporal 정보 까지도 attention mechanism을 이용해서 학습을 했습니다. 이렇게 spatial dependency를 학습하는 방법들의 단점은 spatial dependency를 계산할 때 모든 노드들에 대해서 계산해야 하므로 $O(n^2)$정도의 비용이 필요하다는 점입니다.9. Application9.1. Computer visionComputer vision에서는 scene graph generation, point cloud classification, action recognition 에서 사용이 됩니다.Scene graph generation model은 물체사이에 의미적인 관계를 semantic graph로 나타내는 것을 목표로 합니다. 그리고 scene graph가 주어졌을 때 실제 이미지를 생성하는 방식도 존재합니다.Point cloud를 분류하고 segmentation하는 작업을 통해 LiDAR 장치는 주위 환경을 인식할 수 있게 되었습니다. Point cloud를 k-nearest neighbor graph혹은 super point graph로 변환하거나, ConvGNN을 통해 point cloud의 topological structure를 조사할 수도 있습니다.비디오에서는 사람의 관절을 graph 형태로 표현해서 사람의 행동을 time series graph로 표현할 수 있습니다. 각 관절이 node이고 뼈를 통해 연결이 된 것을 edge로 표현해서 graph를 생성합니다. 이렇게 time series graph 데이터를 통해서 사람의 행동을 인식할 수 있는 모델을 학습할 수 있습니다.9.2. Natural Language ProcessingNLP에서 GNN은 주로 text classification에 사용이 됩니다. 단어나 문서간의 상호관계를 GNN을 통해 파악하는 역할로 사용합니다. 언어 데이터가 순차적인 특성을 갖고있지만, 문장 하나에서도 단어마다 서로 관련이 있기 때문에 graph 구조로 표현이 가능합니다. 문법 구조를 표현하는 graph를 학습하거나 아니면 추상적인 단어들로 구성된 semantic graph에서 같은 의미를 가진 문장들을 생성할 때 GNN을 사용합니다.9.3. Traffic정확하게 도로의 교통량, 혼잡도, 차량의 속도를 예측하는 것은 교통시스템을 설계하는데 있어서 중요합니다. 몇몇 논문들은 traffic network를 spatial-temporal graph로 간주를 해서 traffic을 예측했는데, 도로위의 센서들을 노드, 센서 사이의 거리를 edge, 각 센서의 평균 traffic speed를 dynamic input feature로 삼아서 traffic speed를 STGNN을 이용해서 예측했습니다.9.4. Recommender Systems그래프 기반 추천 시스템은 아이템과 유저를 노드로 간주합니다. 그리고 아이템-아이템, 아이템-유저, 유저-유저 사이의 관계를 graph 형태로 표현해서 좀 더 성능이 좋은 추천을 가능하게 했습니다. 주로 GNN을 통해 아이템과 유저 사이에 link prediction을 통해 아이템을 추천합니다.9.5. Chemistry화학분야에서 연구자들은 GNN을 사용해서 분자나 혼합물의 그래프 구조를 예측합니다. 원자를 노드로 간주하고 화학적인 결합을 edge로 생각해서 molecular fingerprint를 학습하거나, protein interface를 예측하거나 새로운 화학적 혼합물을 합성하는데 사용합니다.10. Future Directions10.1. Model DepthLi et al., 2018 에서 ConvGNN의 성능은 graph convolution layer 수가 많아질수록 감소한다고 밝혔습니다. Graph convolution은 인접 노드가 서로 유사해지도록 만드는데, graph convolution이 무한대만큼 존재하면 결국 모든 노드들이 하나의 representation으로 수렴하게 될 것이라고 주장하고 있습니다. 그래서 그래프 데이터를 학습할 때 깊이를 어느정도로 설정해야할지에 관해서 아직까지 의견이 분분합니다.10.2. Scalability trade-offGNN의 scalability는 이웃노드를 sampling 하거나 clustering 하는 것을 통해 확보합니다. 이웃노드를 sampling 하면 계산하는 비용은 감소하지만 중요한 정보를 포함하고 있는 이웃 노드의 정보를 반영하지 못하게 됩니다. 또한 clustering을 통해 중요한 구조적인 패턴 정보를 손실하게 됩니다. 그래서 정보를 많이 일지 않고 계산량을 줄이는 방법에 관해서도 연구가 필요합니다.10.3. Heterogenity대부분 GNN은 homogeneous graph를 가정합니다. 그래서 현재 GNN을 heterogenous graph에 적용하는 것은 어렵습니다. Heterogenous graph는 다른 타입의 노드나 엣지가 포함된 그래프입니다. 예를들어 이미지나 텍스트를 모두 입력으로 받는 그래프 같은 것입니다. 그래서 heterogenous graph에 GNN을 적용하려는 연구도 필요합니다.10.4. DynamicitySTGNN을 통해 graph의 dynamicity가 부분적으로 다루어졌지만, dynamic spatial relation의 경우에 graph convolution을 어떻게 적용해야할지에 관해서 고려하는 연구가 적어서, 이 방향의 연구도 필요합니다."
  },
  
  {
    "title": "Recent Advances on Neural Network Pruning at Initialization 정리",
    "url": "/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/",
    "categories": "Model Compression, Pruning",
    "tags": "Model Compression, Pruning",
    "date": "2023-01-01 00:00:00 +0900",
    





    
    "snippet": "1. Introduction기존의 Pruning 기법은 pretrained model에 적용하는 방법이 대부분이었습니다. 하지만 최근에는 임의로 초기화된 네트워크 (a randomly initialized network)에 pruning 기법을 적용하는 방법들이 연구되고 있습니다. 이 기법을 Pruning at Initialization (PaI) 라...",
    "content": "1. Introduction기존의 Pruning 기법은 pretrained model에 적용하는 방법이 대부분이었습니다. 하지만 최근에는 임의로 초기화된 네트워크 (a randomly initialized network)에 pruning 기법을 적용하는 방법들이 연구되고 있습니다. 이 기법을 Pruning at Initialization (PaI) 라고 하는데, 이 논문에서는 PaI 기법들에 대해 정리하고 있습니다.보통 Pruning pipeline은 3단계로 이루어져 있습니다.(1) Pre-training a dense model.(2) Pruning the dense model to a sparse one.(3) Fine-tuning the sparse model to regain performance.이처럼 Pruning은 pre-trained dense model에 적용하는 post-processing 기법 (Pruning after Training, PaT) 이었는데, 최근에는 임의로 초기화된 네트워크에 적용되는 기법 (PaI)이 연구되고 있습니다. 이전에는 Sparse network를 처음부터 학습했을 때 성능이 안 좋았다는 연구가 많았는데, 최근에는 lottery ticket hypothesis (LTH)와 SNIP 논문에서 sparse network를 처음부터 학습시켰을 때, 원래 network와 성능이 유사했다는 연구결과가 발표되었습니다. 이 논문 이후로 처음부터 sparse network를 학습시키는 방법이 연구되고 있습니다.2. Background of Neural Network Pruning2.1. Classic Topics in Pruning어떤 모델에 Pruning을 적용할 때 중요한 4가지 질문이 있습니다. 어떤 구조로 pruning 할건지, 얼마나 pruning 할건지, 어떤 기준으로 pruning을 할건지, 마지막으로 pruning 과정을 어떻게 스케쥴링할 것인지 입니다. 이거에 대해서는 여기서 간단히만 다루고, 자세한 내용은 이 논문을 참조하시면 됩니다.2.1.1. Sparsity StructurePruning을 특정 pattern에 따라 적용하는 Unstructured Pruning이 있고, pattern을 신경쓰지 않고 단일 weight에 적용하는 Structured Pruning이 있습니다. Structured Pruning은 channel, filter, block 등 특정 단위로 적용하는 방법입니다.Unstructured Pruning은 주로 model size 압축에 사용이 되고, Structured Pruning은 모델 가속에 사용이 됩니다. 왜냐하면 Structured Pruning을 통해 weight을 block 단위로 메모리에 하드웨어에 맞게 저장해서 속도를 빠르게 할 수 있습니다.2.1.2. Pruning RatioPruning Ratio는 얼마나 많은 weight를 제거할지 결정하는 값입니다. 일반적으로 ratio를 결정하는 방법이 두가지가 있습니다.  미리 정의하는 방법모든 layer를 하나의 ratio 값으로 pruning 하는 방법이 있고, layer 마다 ratio를 다르게 주는 방법이 있습니다.  다른 방법을 통해 결정하는 방법Regularization 같은 방법을 통해 간접적으로 pruning ratio를 결정할 수 있습니다.하지만 특정 Sparsity에 도달하기 위해서 ratio를 얼마나 설정해야할지 결정하기 위해서는 많은 tuning 과정이 필요합니다. 그리고 미리 정의하는게 좋은지, 자동으로 찾는게 좋은지에 관해서도 의견이 분분해서 아직까지 합의점은 찾지 못했습니다.2.1.3. Pruning CriterionPruning criterion은 어떤 기준으로 weight을 제거할지 결정합니다. 이 문제는 pruning에서 가장 중요해서 많은 Pruning 연구들이 criterion 중심으로 진행되고 있습니다. 가장 단순한 방법으로는 weighted magnitude ( $L_1$-norm for a tensor)를 기준으로 제거하는 것인데, 이 방법이 간단해서 PaT에서 많이 사용되고 있습니다. 대부분 Criterion 설계방식은 특정 weight를 제거했을 때 loss 변화가 가장 적은 파라미터를 선택하는 걸 중점적으로 생각합니다.Criterion 관련해서 많은 연구가 진행됐지만, 어떤 방법이 다른 방법보다 확실하게 낫다고 말하기가 어렵습니다. 이에 관해서는 PaT 논문에서 많이 논의가 되었기 때문에 여기서는 넘어가도록 하겠습니다.2.1.4. Pruning Schedule위에 3가지가 결정된 후에 pruning scheduling 방법을 결정하는데, 세가지 종류가 있습니다.(1) One-shot: 단일 스텝으로 한번에 ratio만큼 pruning을 진행하고, fine-tuning을 하는 방법입니다.(2) Progressive: 점진적으로 network sparsity를 0에서 ratio만큼 증가시키고, fine-tuning을 적용하는 방법입니다.(3) Iterative: Network sparsity를 0에서 ratio의 중간지점까지 만든 후에 fine-tuning을 하고, 그 후에 Network sparsity를 중간지점에서 target ratio까지 만든 후에 다시 fine-tuning을 적용하는 방법입니다.보통 (2)와 (3)을 pruning interleaved with network training이라고 부르는데, 두 개사이에 기본적인 차이점을 없어서 어떤 논문에서는 progressive와 iterative를 같은 의미로 사용합니다.그리고 같은 수의 weight을 pruning 할 때, (2)와 (3)이 one-shot보다 성능이 좋다는 것을 대부분 동의합니다. 왜냐하면 (2)와 (3)이 더 많은 iteration을 필요로하기 때문입니다.2.2. A generic Formulation of Pruning먼저 Pruning Paradigm을 수학적으로 정의하고, 그 정의를 바탕으로 PaT와 PaI를 비교하겠습니다. 파라미터가 $\\mathbf{w}$인 neural network를 학습시킬 때 $K$번째 iteration에 모델이 수렴한다고 가정하면, 파라미터 시퀀스를 다음과 같이 나타낼 수 있습니다.\\[\\{\\mathbf{w}^{(0)},\\mathbf{w}^{(1)}, ..., \\mathbf{w}^{(i)}, ..., \\mathbf{w}^{(K)}\\}\\]Sparse network에서는 sparse 구조를 mask로 표현이 가능합니다. 특정 dataset $\\mathcal{D}$를 갖고 Pruning을 수행한다고 하면, 마스크를 다음과 같은 함수로 나타낼 수 있습니다.\\[\\mathbf{m} = \\mathbf{f_1}(\\mathbf{w}^{(k_1)};\\mathcal{D})\\]그리고 pruning 하는 과정에서 기존 파라미터가 조금 변할 수 있는데, 그걸 다음과 같은 함수로 나타낼 수 있습니다.\\[\\mathbf{f_2}(\\mathbf{w}^{(k_2)};\\mathcal{D})\\]마지막으로 pruning을 하고 나서 원래 모델의 performance와 성능을 유사하게 만들기 위해서 fine-tuning 과정을 하는데, 그 과정을 다음과 같은 함수로 표현이 가능합니다.\\[\\mathbf{f_3}(\\mathbf{w}';\\mathcal{D})\\]위에 식 3개를 합쳐서 Pruning의 전체 과정을 다음과 같이 나타냅니다.\\[\\begin{aligned}\\mathbf{w}^{\\prime} &amp; =\\mathbf{f}_{\\mathbf{1}}\\left(\\mathbf{w}^{\\left(k_1\\right)} ; \\mathcal{D}\\right) \\odot \\mathbf{f}_{\\mathbf{2}}\\left(\\mathbf{w}^{\\left(k_2\\right)} ; \\mathcal{D}\\right), \\\\\\mathbf{w}^* &amp; =\\mathbf{f}_{\\mathbf{3}}\\left(\\mathbf{w}^{\\prime} ; \\mathcal{D}\\right)\\end{aligned}\\]마지막에 얻어진 $\\mathbf{w}^*$ 값이 최종적인 pruned model이고, $\\mathbf{f_1}$과 $\\mathbf{f_2}$가 다른 iteration $k_1, k_2$를 사용할 수 있다는 점을 주목해야 합니다. PaI는 Sparse Training과 Sparse Selection 기법으로 나누어지는데, 위에 정의를 바탕으로 PaT, Sparse Training, Sparse Selection을 구분하면 다음과 같습니다.  Pruning after Training (PaT): $k_1 = K, k_2 = K$      Sparse Training (PaI): $k_1 = K$ (LTH) or 0 (SNIP), $k_2 = 0 \\,\\,;\\, \\mathbf{f_2}=\\mathbf{I}$ (identity function), $\\, \\mathbf{f_3} \\neq \\mathbf{I}$    Sparse Selection (PaI): $k_1=k_2=0$, and $\\mathbf{f_2}=\\mathbf{f_3}=\\mathbf{I}$위에 나온대로 PaT는 초기 모델이 pre-training 과정을 거쳐서 수렴했을 때 (K번째 iteration) 얻은 mask와 weight를 그대로 사용해서 fine-tuning 과정을 거쳐서 pruning을 수행합니다.PaI 중에서 Sparse Training은 LTH와 SNIP으로 나누어지는데, LTH에서는 pre-training에서 얻은 mask를 사용하지만 weight는 pre-training 이전 초기값을 사용하고, SNIP에서는 mask와 weight 모두 초기값을 사용합니다. 이후에 추가적으로 $\\mathbf{f_3}$에서 fine-tuning 과정을 적용하는 게 Sparse Training의 특징입니다. Sparse Selection은 mask와 weight 모두 초기값을 사용하고, fine-tuning도 하지 않습니다.지금까지 말한 내용을 정리하면, PaI와 PaT를 구분하는 기준은 수렴한 network의 weight를 그대로 사용할지 아니면 초기화할지 여부이고, Sparse Training과 Sparse Selection을 구분하는 기준은 fine-tuning 유무입니다. 이를 표로 정리하면 다음과 같습니다.이제 LTH와 SNIP에 대해 간단히 살펴보고, Sparse Training과 Sparse Selection에 대해서 자세히 소개하겠습니다.3. Pruning at Initialization (PaI)3.1. Overview: History Sketch  LTH and SNIPPaI가 주목받게된 이유는 PaT에서 필요한 pre-training과 복잡한 pruning schedule이 필요 없어서 pruning을 더 적은 계산비용으로 간단하게 수행할 수 있기 때문입니다. 기존에는 임의로 초기화된 sparse network를 dense network와 같은 성능으로 학습할 수 없다는 결과가 많았는데, LTH와 SNIP 두 논문에서 sparse network로도 dense network만큼 충분히 좋은 성능을 낼 수 있다는 것을 보여주었습니다. 두 방법의 차이점은 LTH는 pre-trained model에서 mask를 얻는다는 점이고 (post-selected mask), SNIP은 임의로 초기화된 모델에서 마스크를 얻는다는 점입니다 (pre-selected mask).  Follow-ups of LTH and SNIPLTH와 SNIP 이후에는 크게 두 가지 방향으로 연구가 진행되고 있습니다. 첫번째 방향은 LTH를 더 큰 dataset과 큰 model에 (e.g., ResNet50 on ImageNet) 대해서 적용하거나 NLP같은 non-vision domain에 적용하고, LTH의 이론적인 기반을 제안하는 방향입니다. 어떤 사람들은 LTH에 비판적인 입장을 갖고 유효성 검사를 하는 방향으로 연구를 진행합니다. 두번째 방향은 SNIP에서 사용되는 pruning criteria를 더 개선하는 쪽으로 연구가 진행되고 있습니다.  Dynamic masksLTH와 SNIP에서 mask는 학습하는동안 고정되어 (static mask) 있습니다. 어떤 연구자들은 학습하는 동안 dynamic and adaptive mask를 선택하는 게 학습을 더 잘하게 만든다고 주장합니다. 그래서 static과 dynamic mask 두가지 모두 연구되고 있는데, 이 방법들을 합쳐서 Sparse Training이라고 부릅니다  Sparse Selection연구자들이 LTH를 실험을 통해 이해하려고 시도하는 과정에서 신기한 사실이 하나 발견되었는데, LTH에서 wining ticket에 의해 선택된 sparse network는 추가적인 학습없이 좋은 성능을 낸다는 것이었습니다 (Strong LTH). 이 사실로 인해서 추가적인 학습 없이 sparse network를 이용하는 방향으로 연구가 진행되고 있는데, 이 방향을 Sparse Selection이라고 부릅니다.현재 PaI 연구 방향은 Sparse Training과 Sparse Selection으로 나누어집니다. 이걸 Tree 형태로 나타내면 다음 그림과 같습니다.3.2. Sparse Training3.2.1. Static masks: post-selectedLottery Ticket Hypothesis (LTH)에서 주장하는 것은 다음과 같습니다.  임의로 초기화된 dense network는 sub-network (winning tickets)를 포함하는데, 해당 sub-network를 비슷한 iteration으로 혼자 학습했을 때 (trained in isolation) test data에 대해서 original network와 견줄만한 성능에 도달할 수 있다LTH에서 Pruning은 3단계로 나누어집니다.(1) 임의로 초기화된 네트워크를 수렴할 때 까지 학습한다.(2) Magnitude pruning을 사용해서 mask를 얻는다.(3) 네트워크를 다시 초기화하고 (2)에서 얻은 mask를 사용해서, 모델을 다시 학습한다.(1), (2), (3) 과정이 iterative pruning 형태로 반복될 수도 있습니다.LTH 영향을 받아서 Sparse Training이라는 연구분야가 생겼는데 이 분야는 두가지로 나뉩니다. 하나는 LTH의 정당성을 규명하거나 더 확장하는 방향이고, 다른 하나는 LTH에 대해 의심을 하면서 유효성 검사를 하는 방향입니다. 이거에 해당하는 것이 Figure 1 Tree 구조에서 Extensions과 Sanity-checks입니다.Extensions초기 LTH 방법은 MNIST 혹은 CIFAR-10 같은 작은 데이터셋에서 검증을 했습니다. 이후에 큰 데이터셋으로 확장해서 적용할 때, 초기 weight를 사용하지 않고 몇 epoch 하급한 후에 weight를 사용해서 ResNet50을 갖고 ImageNet에 적용했습니다. 또한 [Yu et al., 2020] 에서는 NLP와 RL에서도 lottery ticket 현상이 일어난다는 것을 발견해서, 성능을 유지한 채 transformer를 압축하는데 LTH를 사용했습니다. [Chen et al., 2021b]는 LTH를 사용해서 GNN에서 adjacency matrix와 모델 파라미터를 동시에 pruning하는 방법을 제시했습니다.LTH의 pipeline인 train-prune-retrain 과정이 비용이 많이 들어서, [Zhang et al., 2021b]는 Pruning Aware Critical (PrAC) set을 도입했는데, 이 데이터셋은 학습 데이터의 35~80%정도의 양이고 이를 이용해서 학습 iteration을 60~90% 정도 단축시켰습니다. 게다가, PrAC set은 다른 네트워크에 대해서도 적용이 가능하다는 걸 발견했습니다. 유사하게 E-LTH [Chen at al., 2021c]에서는 다른 네트워크로 일반화가 가능한 winning ticket을 찾으려고 시도했습니다. 하나의 network에서 찾은 sparse 구조를 같은 family에 속하지만 depth나 width가 다른 네트워크에 적용해서 비슷한 성능을 내는데 성공했습니다. 또한 [You et al., 2020]는 학습 초기단계에서 비용이 적은 학습 전략으로 winning ticket을 빠르게 찾는 방법을 제시했습니다. 이 방법을 사용해서 4.7배정도 에너지를 절약하면서 비슷한 성능을 유지하는데 성공했습니다.그 외에도 LTH를 이론적으로 이해하려는 시도들이 있었습니다. [Evci et al., 2020b]는 LTH가 왜 가능한지를 gradient flow를 통해 설명했고, [Zhang et al., 2021a]는 LTH의 정당성을 동역학계 이론 (dynamical systems theory)과 inertial manifold 이론을 통해 정당화 했습니다.요약하면 LTH를 확장시키려는 시도들은 주로 LTH + X (LTH를 다른 task, 다른 학습 setting으로 확장), 더 적은 비용으로 ticket을 얻는 방법, LTH에 대한 심도깊은 이해로 이루어졌습니다.Sanity-checksLTH의 유효성은 실험 환경에 많이 영향을 받습니다. 이런 이유로 인해서 LTH에 관해서 논쟁이 많았습니다. [Gale et al., 2019]는 LTH 실험 결과를 재현할 수 없다고 보고했고, [Liu et al., 2021]는 학습률이 크지 않을 때 LTH에서 초기 weight와 마지막 weight 사이에 상관관계가 있다는 걸 발견했습니다. 이 결과를 기반으로 그들은 “winning ticket은 DNN pre-training이 충분하지 않았을 때만 존재한다. 잘 학습된 DNN에서는 winning ticket이 존재하지 않는다.”고 결론지었습니다. 앞선 연구의 영향을 받아서 [Ma et al., 2021]은 learning rate, training epoch, capacity, residual connection 같은 hyper parameter 값에 따라 winning ticket이 존재여부가 결정된다는 구체적인 증거를 제시하고, LTH를 적용할 때 하이퍼 파라미터 선택에 대한 가이드라인을 제시했습니다.3.2.2. Static masks: Pre-selectedStatic masks는 SNIP의 영향을 받아서 탄생했습니다. SNIP에서는 pruning criterion 기준을 connectivity sensitivity라고 부르는데, 이것이 의미하는 바는 특정 weight를 제거했을 때 loss 변화가 가장 적은 weight를 제거하자는 것입니다. 이 방법은 네트워크를 처음에 초기화한 이후에 각 weight 값을 pruning criterion에 따라 할당하고, 이후에 학습을 하면서 pruning ratio만큼 weight를 제거합니다. 이후에 [Wang et al., 2020]는 학습 초기에 loss 값보다는 training dynamics가 중요하다고 주장하면서 gradient signal preservation (GraSP)라는 방법을 제시합니다. 이 방법은 이전에 Hessian 기반으로 loss를 통해 제거할 weight을 판단하는 방법과는 다릅니다.이와는 별개로 [Lee et al., 2020] 에서는 SNIP의 feasibility를 signal propagation을 통해 설명했습니다. 그들은 pruning을 적용하면 네트워크의 dynamical isometry를 손상시킨다는 증거를 찾아내서, 데이터와 독립적으로 초기화하는 방법인 approximated isometry (AI)를 제시했습니다. 이외에도 각 task별로 mask를 선택하는 방법 같은 것이 연구되었습니다.정리하자면 PaT와 유사하게 Static mask에서는 pruning criterion를 어떻게 설정할 것 인지에 관한 연구가 진행되고 있습니다.3.2.3. Dynamic masksSparse Training의 다른 유형으로는 학습 과정중에 mask를 변경할 수 있는 방향입니다. DeepR에서는 학습 도중에 stochasitc parameter를 통해 mask를 parameterization했고, SET에서는 magnitude pruning을 적용하면서 임의로 네트워크의 깊이를 조정하는 방식을 제시했습니다. DSR에서는 하이퍼 파라미터 없이 layer마다 sparsity 비율을 adaptive 방식으로 할당했습니다.Dynamic masks의 본질적인 아이디어는 값이 0이된 파라미터가 다시 학습에 사용될 수 있다는 점입니다. 이 아이디어는 PaT에서도 있었다가, PaI에 적용되기 시작했습니다. 학습 과정중에 mask가 주기적으로 바뀌기 때문에 pruning criterion 비용이 크면 안됩니다. 그래서 모든 criteria는 학습 과정에서 쉽게 이용이 가능한 magnitude 혹은 gradient 기반으로 설계되었습니다.3.3. Sparse SelectionSparse Training은 dense model에서 sub-network를 선택한 후에 추가적인 fine-tuning 과정이 필요합니다. Sparse Selection에서는 파라미터를 최적화 하는 대신에, network topology를 최적화 하는 방향으로 연구가 진행되고 있습니다. 여기서 해결하려는 문제는 다음과 같습니다.  Dense 네트워크가 임의로 초기화되었을 때, 추가적으로 학습할 필요가 없는 sub-network를 찾자.이런 연구 방향은 [Zhou et al., 2019] (Deconstruct)에서 LTH를 이해하려고 시도하다가, 추가적인 학습 없이 높은 accuracy를 확보한 sub-network를 우연히 발견한 이후로 시작되었습니다. 이것이 의미하는 바는 network 파라미터는 임의의 값이지만, 선택된 sub-network 구조는 임의로 선택된 것이 아니라는 것입니다. 다시말해서, sub-network를 찾는 과정이 학습의 일종이라는 의미입니다. 그래서 Zhou는 이를 supermasks라 부르고, mask를 최적화해서 supermasks를 찾는 알고리즘을 제시했습니다.Zhou 방법은 MNIST 혹은 CIFAR 같은 작은 dataset에서 검증되었는데, 이후에 [Ramanujan et al., 2020] 에서는 random Wide ResNet50과 ImageNet에 적용을 했습니다. Ramanujan은 각 파라미터마다 trainable score를 도입해서 loss function을 최소화 하는 방향으로 score를 업데이트 했습니다. Trainable score는 네트워크 구조를 찾는데 사용이 됩니다. 이 방법을 ResNet50에 사용해서 ResNet34보다 작은 sub-network을 선택했는데, 해당 sub-network가 ImageNet을 기준으로 학습된 일반적인 ResNet34보다 top-1 accuracy 측면에서 나은 성능을 보였습니다. 이 연구 결과를 기반으로 논문에서는 LTH의 강력한 버전인 strong LTH를 제시합니다.  임의로 초기화된 over-parameterized network에는 좋은 성능을 가진 (competitive accuracy) sub-network가 존재한다.위의 연구에 영향을 받아서 [Malach et al., 2020]는 strong LTH의 이론적인 근거를 제시하면서 “임의로 초기화된 네트워크를 pruning 하는 것이 학습을 통해 weight를 최적화 하는 것만큼 강력하다”고 주장했습니다.4. Summary and Open Problems4.1 Summary of Pruning at InitializationSparse Training: Much overlap between PaI and PaT.PaT의 주요한 목적은 효율적인 inference 이지만, PaI의 주요한 목적은 효율적인 학습입니다. 이런 차이가 있음에도 불구하고, PaI에서 사용되는 방법은 PaT의 영향을 많이 받았습니다. 2.1에서 다룬 pruning의 4가지 중요한 측면에서, pruning ratio, criterion, schedule 부분은 PaI와 PaT 거의 차이가 없습니다. 하지만 sparsity structure 측면에서는 차이가 있습니다.Structured Pruning은 PaI에서 관심대상이 아닙니다. 왜냐하면 layer마다 sparsity ratio를 주어서 PaI를 적용하는 것은 dense network를 처음부터 학습하는 것으로 귀결되기 때문입니다. 그래서 PaI 연구는 대부분 unstructured pruning 기반으로 진행이 되고있고, 잘알려진 LTH 또한 unstructured pruning에 대해서만 적용이 됩니다. Filter 기반 pruning에 적용되는 이론의 정당성을 증명한 연구는 알려지지 않았는데, 그 이유는 명확하지 않습니다.Sparse Selection: Significant advance of PaI vs. PaT.PaT를 적용할 때 밑바탕이 되는 믿음은 초기 모델에서 학습한 지식을 남아있는 파라미터가 보유하고 있다는 것입니다. 지식을 상속하는 것이 처음부터 학습하는 것보다 좋다고 많은 연구결과가 증명하고 있기 때문에 이런 믿음이 있습니다.이와는 대조적으로 PaI에서는 초기 단계의 파라미터를 이용해서 지식을 상속합니다. 그러면 초기 단계에 있는 파라미터는 충분한 지식을 보유하고 있다는 것일까요? 더 근본적으로는 뉴럴네트워크가 무에서 지식을 학습하는 것인지, 아니면 모델이 원래 소유하고 있던 지식을 드러내는 것인지에 관해서도 질문을 할 수 있습니다. PaT는 전자를 암시하고, PaI는 후자를 암시합니다. 저자는 이런 질문들에 대한 답을 통해 deep neural network에 대해서 좀 더 이론적인 이해로 인도할 수 있다고 믿고있습니다.4.2. Open ProblemsSame problems as pruning after trainingPaI와 PaT가 겹치는 부분이 많기 때문에, PaT에서 있는 open problem들이 PaI에도 똑같이 적용이 됩니다. 2.1에서 말한 4가지 주제에 관해서 계속 연구되고 있습니다.Under-performance of PaIPaI의 아이디어는 실용적인 관점에서 매력적이지만, 여전히 PaT보다 성능이 좋지 않습니다. 예를들어 [Wang et al., 2020] 에 따르면 SNIP과 GraSP는 Cifar-10/100에 대해서 VGG19와 ResNet32에 적용했을 때 전통적인 pruning 방법인 OBD와 MLPrune보다 성능이 꾸준하게 좋지 않다 (해당 방법이 심지어 SOTA도 아닙니다)는 연구결과가 있습니다.Under-development of sparse librariesSparse training의 잠재적인 가능성에도 불구하고, 실용적으로 구현하지 못했습니다. SNFS에서는 5.61배 빠르게 학습을 할 수 있다고 주장했지만, sparse matrix multiplication이 개발이 많이 되지않아서, 빠르게 학습하는 이점을 실제로 적용하지 못하고 있습니다. 이 논문 저자들 지식을 기준으로, sparse training을 사용해서 실제로 학습 시간을 단축시켰다는 (wall-time speedup) 연구결과가 거의 없다고 합니다. 그래서 sparse training library를 개발하는 것이 중요하다고 볼 수 있습니다.5. Conclusion이 논문에서는 PaI 기법에 대해서 정리했습니다. PaI 기법에 대해서 Sparse Training과 Sparse Selection으로 나누어서 정리를 했고, PaT와 비교를 통해 어떤점이 다른지 설명했습니다."
  },
  
  {
    "title": "A survey of Quantization Methods for Efficient Neural Network 정리",
    "url": "/posts/A-survey-of-Quantization-Methods-for-Efficient-Neual-Network-Inference/",
    "categories": "Model Compression, Quantization",
    "tags": "Model Compression, Quantization",
    "date": "2022-12-24 00:00:00 +0900",
    





    
    "snippet": "1. Neural Network 최적화 연구방향들1.1.  효율적인 네트워크 설계Micro-architecture 관점에서는 kernel type을 depth-wise convolution 혹은 low-rank factorization을 사용하는 방법이 있고, Macro-architecture 관점에서는 residual, inception 같은 net...",
    "content": "1. Neural Network 최적화 연구방향들1.1.  효율적인 네트워크 설계Micro-architecture 관점에서는 kernel type을 depth-wise convolution 혹은 low-rank factorization을 사용하는 방법이 있고, Macro-architecture 관점에서는 residual, inception 같은 network를 사용하는 방법이 있다. 이런 방향들의 연구는 대부분 새로운 architecture를 발견하는 것이기 때문에, 작업이 수동적으로 이루어져서 확장가능(Scalable)하지 않다. 그래서 최근에는 Automated Machine Learning (AutoML)이나 Neural Architecture Search (NAS) 처럼 자동으로 효율적인 architecture를 찾는 방법들이 연구되고 있다.1.2. 하드웨어를 고려한 네트워크 설계Latency와 energy 관점에서 네트워크의 오버헤드는 하드웨어 영향을 많이 받는다. 이런 방향도 처음에는 수동적으로 했지만 점차 자동으로 변하고 있다.1.3. Pruningmemory 사용량과 계산비용을 줄이는 방법 중 pruning이 있다. Pruning은 중요하지 않은 뉴런 (neurons with small saliency)을 제거해서 sparse computational graph를 생성한다. Pruning은 크게 두 가지 방법으로 나뉜다.  Unstructured PruningNeuron 단위로 파라미터를 제거하는 방법. 이런 방법은 sparse matrix 연산으로 이어지게 되는데, 이 연산은 가속화하기 어렵고 연산속도가 메모리에 의해 결정된다고 (memory-bound) 알려져있다.  Structured Pruning그룹 단위로 파라미터를 제거하는 방법. 각 layer의 input과 output shape를 변화시킨다. 하지만 너무 많이 할 경우 performance가 떨어진다는 단점이 있다.1.4. Knowledge Distillation커다란 모델을 훈련하고 해당 모델을 teacher로 삼아서 작은 모델을 훈련하는 방법. Teacher에의해 생성어된 soft probability를 이용해서 훈련을 한다. Distillation 방법은 quantization이나 pruning에 비해 압축율 대비 성능이 낮다. 그래서 주로 quantization이나 pruning과 결합해서 사용이 된다.1.5. Quantization네트워크를 학습하거나 추론할 때 꾸준히 좋은 성공을 거두는 방법입니다. 이 논문에서는 대부분 inference에 관한 quantization 방법이지만, quantization 기법은 training 기법에서 성공을 더 많이 거두었다고 합니다. 특히 half-precision이나 mixed-precision training 기법이 네트워크를 가속하는 주요한 방법입니다. 하지만 half-precision보다 더 밑으로 학습하는 방법은 세세한 tuning 방법이 필요해서, 최근 연구 방향은 inference 측면에 초점을 두고 있습니다.2. Quantization 기법 종류문제정의일반성을 잃지않고 Supervised Learning이라고 가정을 하자. 뉴럴 네트워크가 L개의 layer를 갖고있고, 각 layer의 파라미터를 $\\theta = {W_1,W_2,…,W_L}$ 라고 했을 때 loss function을 다음과 같이 정의할 수 있다.\\[\\mathcal{L}(\\theta)=\\frac{1}{N} \\sum_{i=1}^N l\\left(x_i, y_i ; \\theta\\right)\\]Quantization의 목표는 모델의 generalization power / accuracy에 영향을 주지 않은 채로 $\\theta$와 intermediate activation($h$)의 precision을 낮추는 것이다. $\\theta$와 $h$에 어떤 Quantization 함수를 적용하는지에 따라 방법이 나뉜다.2.1. Uniform Quantization\\[Q(r) = Int(r/S)-Z \\tag{1}\\]실수 r을 scale factor S로 나누고, zero point를 맞춰주기 위해 Z를 빼준다. 이러면 위의 그림처럼 실수가 scale factor 간격 (주황색 점을 잇는 선분)마다 속한 값으로 변경이된다.2.2. Symmetric and Asymmetric Quantization위의 (1)번 식에서 scaling factor S에 따라 실수가 대응되는 정수 partition 개수가 달라진다.\\[S=\\frac{\\beta-\\alpha}{2^b-1} \\tag{2}\\]여기서 $[\\alpha,\\beta]$는 clipping range를 나타내는데, 이 범위에 속한 실수값들에 대해서만 quantization을 적용하겠다는 의미이다. Clipping range를 결정하는 과정을 calibration이라고 한다.Symmetric quantization은 $\\alpha = -\\beta$로 두는 방법이다. 이러면 zero point를 조정할 필요가 없어서 Z값이 0이 되고, (1)번식에서 Z를 빼는 추가적인 계산 비용이 소모되지 않는다는 장점이 있다. 하지만 ReLU처럼 output이 한쪽으로 쏠려있는 실수값에 symmetric quantization을 적용하게 되면 asymmetric에 비해 resolution이 작아지게된다. 이런 단점에도 불구하고 뺄셈연산 하나를 줄이는게 inference할 때 계산비용을 줄여줘서 실제로 많이 활용된다.Asymmetric quantization은 $\\alpha = r_{min}, \\beta = r_{max}$로 두는 방법이다. 그래서 symmetric quantization 보다 더 넓은 범위 실수값에 대해서 quantization을 적용할 수 있고, 값이 한쪽으로 치우쳐진 경우에 대해서 더 높은 resolution으로 데이터 표현이 가능하다. 하지만 zero point가 0이 아니기 때문에 quantization 과정에서 추가적으로 Z값을 빼줘야한다.두 Quantization 기법은 모두 outlier에 취약하다. clipping range를 벗어난 입력이 inference할 때 들어올 경우 문제가 된다. 그래서 min/max value로 $\\alpha, \\beta$를 설정하지 않고, i번째로 가장 큰/작은 값으로 설정하거나 실수값과 quantized value 사이에 KL divergence가 최소가되는 $\\alpha, \\beta$를 선택하는 기법들이 있다.2.3. Static and Dynamic QuantizationSymmetric, Asymmetric quantization 기법은 모든 layer에서 $\\alpha, \\beta$ 값이 동일하고 그 값들이 네트워크가 forward pass 되기 전에 미리 계산이 되는 static quantization 기법이다. 하지만 각 layer의 activation map마다 실수범위가 다르고 각 batch마다 값이 다르기 때문에, 값을 미리 하나로 고정시키는 것은 accuracy 측면에서 성능이 떨어진다.그래서 inference 하는 과정에서 각 batch, layer마다 $\\alpha, \\beta$ 값을 결정해주는 방법을 dynamic quantization이라고 한다. 이 방법은 정확도 측면에서 성능이 좋아지지만 계산량이 추가적으로 소모돼서 자주 쓰이지 않는다.2.4. Quantization Granularity컴퓨터 비전에서 convolution 연산을 할 때 layer마다 filter 값의 범위가 다르다. 그래서 네트워크 파라미터를 어떤 단위(Granularity)로 clipping 하는지에 따라 quantization 기법이 나뉘어진다.Layer-wise Quantization은 layer마다 convolutional filter에 있는 모든 파라미터를 고려해서 $\\alpha, \\beta$를 결정한다. 그래서 하나의 layer에 있는 filter들은 동일한 범위로 quantization이 진행된다. 이 방법은 구현하기 간단하지만 종종 정확도측면에서 손실이 발생한다. 왜냐하면 layer 안에서도 필터마다 값의 범위가 다양하기 때문이다. Figure 3을 보면 Layer 1에 있는 Filter 4개의 분포가 모두 상이하다. 하지만 Layer Quantization은 모든 필터에 같은 clipping range를 적용하기 때문에, resolution이 좋지 않은 Filter가 발생한다. (위의 그림에서 Filter 1, Filter 2, Filter C 의 경우 resolution이 좋지 않다.)Group-wise Quantization은 layer 안에서도 필터끼리 묶어서 서로 다른 $\\alpha, \\beta$를 설정한다. 이 방법은 하나의 convolution/activation마다 파라미터의 분포가 상이하게 다른경우에 도움이 된다. 예를들어 Q-BERT 같은 모델에서 이 방법을 사용해서 성능이 2%정도 감소하고 파라미터를 13배나 압축하는 결과를 얻었다.Channel-wise Quantization은 각 filter마다 clipping range를 설정하는 방법이다. 이 방법은 quantization resolution이 좋고 종종 accuracy 측면에서 성능이 좋다. 현재 convolution kernel을 quantization할 때 가장 많이 사용하는 방법이다. Figure 3에 오른쪽 그림을 보면, channel마다 clipping range가 다르다.Sub-channel-wise Quantization은 convolution filter 하나에서도 group을 나누어서 clipping range를 설정하는 방법이다. Resolution은 좋아지지만 계산량이 많아져서 잘 쓰이지 않는다.2.5. Non-Uniform QuantizationUniform quantization은 $[\\alpha, \\beta]$에서 균일한 간격으로 같은 값으로 quantization을 진행헀다. 하지만 간격 크기를 다르게하고, 간격마다 다른 값으로 quantization 하는 기법이 non-uniform quantization이다. 일반적으로 다음과 같이 표현한다.\\[Q(r)=X_i \\text {, if } r \\in\\left[\\Delta_i, \\Delta_{i+1}\\right) \\tag{3}\\]$X_i$는 quantization level, $\\left[\\Delta_i, \\Delta_{i+1}\\right)$는 quantization steps이다. 각 구간마다 다른 level로 quantization이 진행되고 구간마다 길이가 다르다. 이 방법은 fixed bit-width일 때 좋은 성능을 얻을 수 있다. 왜냐하면 중요한 값이 많이 몰려있는 영역에 집중해서 resolution을 높이고, 중요하지 않은 영역의 resolution은 낮추면 되기 때문이다. 예를들어 non-uniform quantization은 대부분 파라미터 값의 분포를 bell-shape로 설정한다. 최근 기법들은 quantizer Q를 학습을 통해 찾아낸다.\\[\\min _Q\\|Q(r)-r\\|^2 \\tag{4}\\]위의 최적화 문제를 풀어서 quantizer를 구한다. 위의 방법 외에도 clustering 방법을 사용해서 파라미터마다 quantizer를 구하는 방법도 있다.non-uniform quantization은 uniform에 비해 더 많은 정보를 표현할 수 있지만, GPU 혹은 CPU에서 효율적으로 연산을 하기 어어려워서 대부분 uniform quantization을 사용한다.2.6. Fine-tuning MethodsQuantization 이후에 네트워크의 파라미터 조정이 필요한 경우가 있다. 이런 경우에 모델을 다시 학습하는 Quantization-Aware Training (QAT) 방법이 있고, 모델을 다시 학습하지 않는 Post-Training Quantization (PTQ)가 있다.2.6.1. Quantization-Aware Training (QAT)Quantization을 적용하면 모델 파라미터에 perturbation을 가한 것이기 때문에 floating point precision으로 학습했을 때 수렴했던 곳과 다른 곳에 위치하게 된다. 그래서 quantized model을 다시 학습해서 loss를 수렴시킬 필요가 있다. QAT는 이런 문제를 해결해주는 방법인데, Quantized weight를 사용해서 forward, backward pass를 한 후에 얻어진 quantized gradient를 floating point로 변환한 후에 gradient를 update한다. 정리하자면 다음 그림과 같다.위에서 $\\frac{dL}{dQ}$를 $\\frac{dL}{dR}$로 변환할 때, Straight Through Estimator (STE)를 사용하는데, STE는 quantized gradient로 real gradient를 근사하는 기법이다. 실제로 Binary Quantization 처럼 너무 낮은 precision quantization 기법을 제외하고, STE 근사 성능이 좋다. STE 이외에도 gradient를 근사하는 기법이 연구되고있다.Gradient 근사 기법외에도 다른 방식으로 QAT를 하는 방법이 있는데, ProxQuant라는 방법은 (1)에서 사용된 rounding operation을 없애고, 파라미터에 non-smooth regularization(W-shape)을 적용해서 quantized value를 얻는 기법을 사용한다. 또 다른 방법으로는 pulse training이 있는데, 이 방법은 불연속점의 derivative를 근사하거나, quantized weight를 floating point와 quantized parameter의 affine combination으로 바꿔서 QAT를 진행한다. 하지만 이런 기법들은 tuning이 많이 필요해서 STE가 일반적으로 많이 사용된다.모델 파라미터를 조정하는 방법 외에도, QAT를 하는 과정중에 quantization parameter를 학습하는 방법이 있다. PACT는 clipping range를 학습하는 방법이고, QIT는 quantization step을 학습한다.QAT는 효과가 좋지만 모델을 다시 학습해야한다는 추가적인 비용이 발생한다. Quantized model이 오랜 기간동안 deploy된다면 QAT를 적용할만하지만, 모델의 라이프사이클이 짧은 경우, QAT가 시간낭비가 될 수 있다.2.6.2. Post-Training Quantization (PTQ)Fine-tuning 없이 quantization을 수행하고 weight를 조정하는 방법이다. PTQ는 QAT에 비해 오버헤드가 거의 없고, re-training을 할 때 많은 양의 데이터가 필요하지 않고, unlabeled data에 대해서도 적용이 가능하다. 하지만 QAT에 비해 accuracy는 낮아진다. 그래서 lower accuracy를 개선하려고 여러가지 방법이 연구되고 있다. 대표적으로 OMSE는 quantized tensor와 floating point tensor 사이에 L2 distance를 줄이는 방향으로 학습을 진행한다.위의 그림은 QAT와 PTQ를 비교한 것이다. 둘의 차이점은 QAT는 학습 데이터 전체가 fine-tuning 과정에서 사용되고, PTQ는 일부 데이터만 Calibration 과정에서 사용이 된다는 점이다.2.6.3. Zero-shot Quantization (ZSQ)앞선 방법들은 quantization 과정에서 학습데이터를 사용했는데, 학습 데이터가 너무 커서 분산되어있거나, 보안같은 문제로 학습 데이터를 이용하지 못하는 경우가 있다. 이 경우에 zero-shot quantization을 사용할 수 있는데, 두 가지 수준으로 나누어진다.  Level 1: No data and No fine-tuning  (ZSQ + PTQ)  Level 2: No data but requires fine-tuning (ZSQ + QAT)Level 1 방법은 weight 범위를 동일하게 맞춰주거나, bias error를 수정하는 방식으로 이루어진다. 이 방법은 linear activation의 scale-equivariance property에 근거한 방법이기 때문에, non-linear activation의 경우 sub-optimal solution을 얻을 가능성이 있다.ZSQ 연구방향중 유명한 갈래 중 하나인 synthetic data를 생성하는 방법이 있다. Pre-trained model을 discriminator로 삼고, 학습 데이터와 유사한 synthetic data를 생성해서 quantization model을 fine-tuning하는 방법이다. 하지만 이 방법은 internal statistics ( distributions of the intermediate layer activations)을 고려하지 않은 방법이기 때문에 실제 데이터 분포를 잘 나타내지 못한다. 그래서 batch normalization에 저장된 statistics를 이용해서 더 실제같은 synthetic data를 생성하는 방법도 있다. 실제 데이터 분포와 유사한 synthetic data를 만들어낼 수록, ZSQ 성능이 좋다는 연구결과들이 있다.2.7. Stochastic QuantizationQuantization 과정에서 Stochastic 성질을 주면 rounding operation으로 인해서 weight 변화가 없을 거라는 생각을 할 수 있지만, stochastic rounding을 통해 네트워크는 탈출(?)할 수 있게 돼서 파라미터 업데이트가 가능하다. 예를들어 다음과 같은 방식으로 Stochastic Rounding 적용이 가능하다.\\[\\operatorname{Int}(x)= \\begin{cases}\\lfloor x\\rfloor &amp; \\text { with probability }\\lceil x\\rceil-x \\\\ \\lceil x\\rceil &amp; \\text { with probability } x-\\lfloor x\\rfloor\\end{cases} \\tag{5}\\]Binary Quantization의 경우는 다음과 같이 사용이 가능하다.\\[\\operatorname{Int}(x)= \\begin{cases}\\lfloor x\\rfloor &amp; \\text { with probability }\\lceil x\\rceil-x \\\\ \\lceil x\\rceil &amp; \\text { with probability } x-\\lfloor x\\rfloor\\end{cases} \\tag{6}\\]최근에는 QuantNoise가 computer vision이나 NLP에서 accuracy 변화가 거의 없이 quantization을 사용했다. 이 방법은 forward pass할 때 마다 임의의 weight subset을 quantize하고 unbiasd gradient로 모델을 학습시키는 방법이다. 하지만 이 방법의 단점은 weight를 update할 때마다 random number를 생성해야하는 오버헤드가 있다는 점이다. 이 때문에 실제로 거의 사용되지 않는다.3. Quantization Below 8 Bits3.1. Simulated and Integer-only QuantizationQuantized model을 배포하는 두 가지 방법이있다. 첫번째는 simulated quantization (fake quantization)인데, 이 방법은 quantized model parameter를 lower-precision으로 저장을 하지만 행렬곱이나 컨볼루션같은 연산을 floating point로 수행한다. 그래서 quantized parameter는 floating point 연산을 하기 전에 dequantized 되어야한다. 그래서 빠르게 연산을 할 수 없다는 단점이 있다.(저장할 때 용량을 줄이기 위해서 이렇게 하는건가?)두번째는 Integer-only quantization이 있는데, 이 방법은 모든 연산이 low-precision integer에서 수행이 돼서 속도가 빠르다. low-precision에서는 latency, power consumption, area efficiency 측면에서 full-precision보다 좋다. 많은 hardware processor들이 기본적으로 빠른 low-precision 연산을 지원한다. 또한, Int 8 덧셈 연산이 FP32 덧셈 연산보다 에너지 관점에서 30배 좋고, area 관점에서 116배 좋다.Dyadic quantization은 integer-only quantization의 한 종류인데, dyadic number(분자가 정수, 분모가 2의 거듭제곱)를 사용해서 모든 scale 연산이 수행된다. 이 방법을 사용하면 나눗셈 없이, integer 덧셈, 곱셈, 비트이동 만으로 모든 연산을 나타낼 수 있다.Integer-only quantization은 속도 측면에서 fake quantization보다 좋다. 하지만 추천시스템처럼 compute-bound보다 bandwidth-bound 문제가 되는 경우에 fake quantization이 효과적이다. 왜냐하면 이런 경우 대부분 병목현상이 메모리 사용량과 파라미터를 메모리에서 로드하는데서 발생하기 때문이다.셋을 정리하자면 다음과 같다.                   Full Precision      Simulated Quantization      Integer-only Quantization                  저장      FP32      INT 4      INT 4              연산      FP 32      FP 32      INT 4              출력      FP 32      INT 4      INT 4      3.2. Mixed-Precision Quantization이 방법은 각 layer를 다른 bit precision으로 quantized하는 방법이다.  이때 결정하는 기준은 각 layer의 quantization에 대한 sensitivity이다. 만약 어떤 layer에 quantization을 적용했을 때 성능 저하가 심하면 (sensitivity가 크면) 해당 layer 파라미터는 higher bit precision으로 quantization을 적용하고, 반대로 어떤 layer에 quantization을 적용했을 때 성능에 변화가 거의 없다면, lower precision bit를 사용하는 기법이다. 이 방법의 단점은 layer 수가 많아질 수록 각 layer 마다 bit를 결정하기 위해 탐색 범위가 지수적으로 증가한다는 점이다. 그래서 탐색 범위를 제한하기 위한 방법들이 제안되고 있다.최근 논문은 강화학습 (RL) 기반으로 quantization policy를 결정하는 방법을 제시한다. Hardware simulator를 agent로 설정해서, hardware 가속이 얼마나 되는지를 기준으로 policy를 결정한다. 다른 논문은 search space 문제를 Neural Architecture Search (NAS) 문제로 간주해서 search space를 찾는다. 이런 방법들은 계산량이 많이 필요하고 초기 변수값과 하이퍼파라미터에 민감하다는 단점이 있다.다른 방법으로는 HAWQ가 있는데, 이 방법은 각 layer에 quantization을 적용했을 때 Hessian matrix를 통해  sensitivity를 측정해서 bit precision을 결정한다. 이 방법은 Pruning에서 중요한 논문 중 하나인 Optimal Brain Damage와 유사하다. HAWQv2에서는 layer의 파라미터 뿐만 아니라 activation까지 quantization을 적용했는데, RL 기반 mixed-precision보다 100배나 빠른 결과가 나타났다. HAWQv3는 integer-only, hardware-aware quantization 방법이 도입되었다. 이 방법은 Integer Linear Programming을 사용해서 optimal bit precision을 결정한다. T4 GPU를 기준으로 mixed-precision (INT4/INT8) quantization을 사용했을 때 INT8 quantization보다 속도가 50배나 향상되었다.3.3. Hardware Aware QuantizationQuantization의 목표 중 하나는 inference latency를 줄이는 것이다. 하지만 quantization을 적용했다고 해서 모든 hardware가 동일하게 inference 속도를 향상시키는 건 아니라, hardware의 on-chip memory, bandwidth, cache hierarchy에 따라 결정된다. 그래서 hardware-aware quantization 방법이 연구되고 있다.3.4. Distillation-Assisted QuantizationAccuracy가 높은 large model에서 생성한 output을 이용해서 quantization을 진행하는 방법이 있다. 이때 어느 부분의 output을 이용하는지에 따라 방법이 나뉘는데, 마지막 layer의 soft probabilities를 사용하거나, 중간 layer의 feature를 사용하는 방법이 있다. Teacher model과 Student model을 각각 생성하는 방법도 있지만,  추가적인 teacher model 없이 self-distillation하는 방법도 있다.3.5. Extreme QuantizationQuantization의 가장 극단적인 경우는 1-bit representation (Binarization)이다. 이 방법은 memory 사용량을 32배 줄여줄 뿐만 아니라, bit-wise arithmetic을 사용해서 binary (1-bit) and ternary (2-bit) 연산을 가속할 수 있다. 예를들어 NVIDIA V100 GPU에서 peak binary arithmetic은 INT8보다 8배 빠르다. 하지만 단순히 binarization을 적용하면 accuracy 감소현상이 심하기 때문에 binarization을 잘하는 방법을 연구하고있다.BinaryConnect는 파라미터를 실수값으로 저장하는데, forward와 backward pass를 할 때만 sign값을 기준으로 +1과 -1을 할당해서 연산을 한다. Sign 함수가 미분 불가능하기 때문에 STE estimator를 사용해서 gradient를 근사한다. Binarized NN은 파라미터 뿐만아니라 activation까지도 binarization을 적용하는 방법이다. 이를 통해 latency를 낮추었다. Binary Weight Network (BWN)은 scale factor를 추가해서 $+\\alpha, -\\alpha$로 binarization을 수행하는 방법이다. 이때 $\\alpha$를 결정할 떄 다음과 같은 최적화 문제를 푼다.\\[\\alpha, B=\\operatorname{argmin}\\|W-\\alpha B\\|^2  \\tag{7}\\]위의 방법으로 학습한 weight가 0에 가깝다는 관찰을 바탕으로, binarization 대신 tenarization (+1, 0, -1)을 적용하는 방법도 있다. 이 방법은 binarization보다 행렬곱 연산속도를 줄여주기 때문에 inference latency가 감소한다.최근에는 BERT, RoBERTa, GPT같은 pre-trained model에 extreme quantization을 적용하는 방법들이 연구되고 있다.Binarization과 Tenarization을 단순히 적용하면 accuracy 감소가 심하기 때문에 ImageNet 분류처럼 복잡한 테스크에서 성능이 좋지 않다. 그래서 이 문제를 해결하려고 3가지 방향으로 연구가 진행되고 있다.  Quantization Error Minimization이 방법은 weight의 실수값과 quantized value 차이를 최소화하는 것을 목표로 한다. real-value weight/activation을 단일 binary matrix로 표현하는 대신, binary matrix의 선형 결합( 식 )으로 표현해서 quantization error를 줄인다.  Improved Loss Function이 방법은 Binarized/ternatized weight에 대해서 loss를 직접 최소화 하는 방법이다. 다른 방법들은 quantized weight가 loss function에 반영되지 않는데, 이 방법은 반영이 된다.  Improved Training MethodSign함수의 gradient를 근사할 때 STE를 사용하는데, STE는 [-1, 1] 사이에 gradient만 전파가능하다는 단점이 있다. 그래서 BNN+에서는 sign 함수의 미분을 연속함수로 근사하는 방법을 도입했고, 다른 방법에서도 sign 함수를 smooth하고 미분가능한 함수로 대체하는 방법을 사용했다.3.6. Vector Quantization디지털 신호 처리 (DSP)에서 역사적으로 quantization과 관련된 연구가 많이 진행됬는데, ML에서 quantization과 접근 방법이 조금 다르다. DSP에서는 신호를 최대한 오류없이 압축하는 방법에 관심이 있는데, ML에서는 파라미터나 activation을 reduced-precision으로 표현하는데 관심이 있기 때문이다.그래도 DSP에서 사용하는 quantization 방법을 ML에 적용하려는 시도들이 있었다. 특히 clustering 방법이 많이 연구 됐는데, weight를 비슷한 값끼리 clustering해서 몇 개의 그룹으로 나눠서 얻은 중심점을 quantized value로 사용하는 방법이다. 실제로 K-means clustering을 사용해서 급격한 accuracy 감소없이 8배나 모델 사이즈를 줄인 성과가 있었다. 게다가 pruning과 Huffman coding을 같이 적용해서 모델 크기를 더 줄이는 연구도 있었다.Product Quantization은 weight matrix를 sub-matrices로 나누고, 각 sub-matrix에 vector quantization을 적용하는 방법이다.4. Future Directions for Research in Quantization4.1. Quantization Software정확성을 잃지않고 INT8 quantization 모델로 배포할 수 있는 소프트웨어 패키지들이 많이 있다. 하지만 lower bit-precision quantization에 관한 소프트웨어는 많이 없다.4.2. Hardware and NN Architecture Co-DesignNN architecture의 width를 변화시키면 quantization 이후에 generalization gap이 줄어든다는 연구 결과가 있다. 그래서 Quantization을 할 때 depth나 kernel 처럼 architecture 구조 파라미터도 같이 학습하는 방법이 연구되고 있다.또는 hardware architecture와 같이 quantization method를 설계하는 연구도 있다. 예를들어 FPGA에서 배포를 할 때 이런 방식으로 설계하면 효과가 좋을 거라 예상한다.4.3. Coupled Compression MethodsQuantization이 다른 방법과 같이 사용될 수 있는데, 어느 경우에 최적의 조합이 되는지에 관한 연구가 거의 없다.4.4. Quantized TrainingQuantization을 학습과정에 적용할 수도 있는데, INT8 precision training을 적용하려면 어려움이 많다고 한다. 보통 하이퍼 파라미터 튜닝이 많이 필요하거나 상대적으로 쉬운 task에만 적용이 된다고 한다. 왜냐하면 INT8 precision을 사용해서 학습을 하면, 학습이 불안정하고 발산할 수 있기 때문이다.Summary  Quantization은 weight와 activation에 적용이 가능하다."
  }
  
]

