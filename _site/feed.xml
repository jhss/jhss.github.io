

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>Juhong Song</title>
  <subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle>
  <updated>2023-02-12T22:54:39+09:00</updated>
  <author>
    <name>juhong</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator>
  <rights> © 2023 juhong </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리</title>
    <link href="http://localhost:4000/posts/8-bit-matrix-multiplication/" rel="alternate" type="text/html" title="LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리" />
    <published>2023-02-12T00:00:00+09:00</published>
  
    <updated>2023-02-12T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/posts/8-bit-matrix-multiplication/</id>
    <content src="http://localhost:4000/posts/8-bit-matrix-multiplication/" />
    <author>
      <name>juhong</name>
    </author>

  
    
    <category term="Model Compression" />
    
    <category term="Quantization" />
    
  

  
    <summary>
      





      1. Introduction

Large pre-trained 언어모델에 대해서 8-bit quantization을 적용하는 기법들이 많이 연구되었지만, 이런 기법들은 350M 이하 스케일에 대해서만 연구되는 경우가 많았습니다. 이 논문에서는 performance 감소없이 billion 단위에서도 적용 가능한 quantization 기법을 제시합니다.

이 논문에서는 파라미터 스케일이 6B정도가 되었을 때 Transformer에만 나타나는 특이한 현상이 있다고 합니다. 트랜스포머 레이어 전체에서 25% 정도에서만 관찰이 된다고 하는데, 특정 차원의 feature 크기가 다른 차원의 feature 크기보다 20배정도 크게 나타난다고 합니다.

Transformer에서만 관측되는 특이한 현상을 분석하기 위...
    </summary>
  

  </entry>

  
  <entry>
    <title>Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 정리 (Chapter 1 ~ 3)</title>
    <link href="http://localhost:4000/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/" rel="alternate" type="text/html" title="Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 정리 (Chapter 1 ~ 3)" />
    <published>2023-01-22T00:00:00+09:00</published>
  
    <updated>2023-01-22T15:06:29+09:00</updated>
  
    <id>http://localhost:4000/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/</id>
    <content src="http://localhost:4000/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/" />
    <author>
      <name>juhong</name>
    </author>

  
    
    <category term="Model Compression" />
    
    <category term="Pruning" />
    
  

  
    <summary>
      





      1. Introduction

현대의 딥러닝 모델은 대부분 크기가 커서 메모리를 많이 차지하고 학습과 추론 단계에서 계산량이 많이 필요합니다. 이런 이유 때문에 모델을 경량화 하려는 연구들이 많이 진행 되었고, 그 중 한가지 연구방향이 Sparsification 입니다. Sparsification은 고차원 feature space에서 몇개의 파라미터 값을 0으로 만들어서 전체가 아니라 일부만 가지고 학습과 추론을 하는 방법입니다. 이 방법을 통해 모델의 complexity를 낮출 수 있습니다. 본 논문에서는 Sparsification을 통해 모델을 경량화하는 여러가지 방법에 대해 정리했습니다.

2. Overview of Sparsity in Deep Learning

Sparsification을 통해...
    </summary>
  

  </entry>

  
  <entry>
    <title>A Comprehensive Survey on Graph Neural Networks 정리</title>
    <link href="http://localhost:4000/posts/A-Comprehensive-Survey-on-Graph-Neural-Networks/" rel="alternate" type="text/html" title="A Comprehensive Survey on Graph Neural Networks 정리" />
    <published>2023-01-08T00:00:00+09:00</published>
  
    <updated>2023-01-11T14:45:46+09:00</updated>
  
    <id>http://localhost:4000/posts/A-Comprehensive-Survey-on-Graph-Neural-Networks/</id>
    <content src="http://localhost:4000/posts/A-Comprehensive-Survey-on-Graph-Neural-Networks/" />
    <author>
      <name>juhong</name>
    </author>

  
    
    <category term="Graph Neural Network" />
    
  

  
    <summary>
      





      1. Introduction

딥러닝은 Euclidean space에서 표현된 데이터에 대해서 성공적인 성과를 거두었지만, 최근에 non-Euclidean space에서 생성된 데이터에 딥러닝을 적용하려는 시도가 많아지고 있습니다. 본 논문에서는 GNN을 4개의 카테고리 (Recurrent GNN, Convolutional GNN, Graph autoencoder, Spatial-temporal GNN)으로 나누어서 소개하고 있습니다.

2. Categorization and Frameworks

먼저 GNN의 taxonomy와 framework에 대해서 간단하게 소개하고, 각각의 요소들을 나중에 자세히 다루겠습니다.

2.1. Taxonomy

Recurrent graph neural networks...
    </summary>
  

  </entry>

  
  <entry>
    <title>Recent Advances on Neural Network Pruning at Initialization 정리</title>
    <link href="http://localhost:4000/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/" rel="alternate" type="text/html" title="Recent Advances on Neural Network Pruning at Initialization 정리" />
    <published>2023-01-01T00:00:00+09:00</published>
  
    <updated>2023-01-01T22:47:39+09:00</updated>
  
    <id>http://localhost:4000/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/</id>
    <content src="http://localhost:4000/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/" />
    <author>
      <name>juhong</name>
    </author>

  
    
    <category term="Model Compression" />
    
    <category term="Pruning" />
    
  

  
    <summary>
      





      1. Introduction

기존의 Pruning 기법은 pretrained model에 적용하는 방법이 대부분이었습니다. 하지만 최근에는 임의로 초기화된 네트워크 (a randomly initialized network)에 pruning 기법을 적용하는 방법들이 연구되고 있습니다. 이 기법을 Pruning at Initialization (PaI) 라고 하는데, 이 논문에서는 PaI 기법들에 대해 정리하고 있습니다.

보통 Pruning pipeline은 3단계로 이루어져 있습니다.

(1) Pre-training a dense model.

(2) Pruning the dense model to a sparse one.

(3) Fine-tuning the sparse model to regain...
    </summary>
  

  </entry>

  
  <entry>
    <title>A survey of Quantization Methods for Efficient Neural Network 정리</title>
    <link href="http://localhost:4000/posts/A-survey-of-Quantization-Methods-for-Efficient-Neual-Network-Inference/" rel="alternate" type="text/html" title="A survey of Quantization Methods for Efficient Neural Network 정리" />
    <published>2022-12-24T00:00:00+09:00</published>
  
    <updated>2022-12-24T00:00:00+09:00</updated>
  
    <id>http://localhost:4000/posts/A-survey-of-Quantization-Methods-for-Efficient-Neual-Network-Inference/</id>
    <content src="http://localhost:4000/posts/A-survey-of-Quantization-Methods-for-Efficient-Neual-Network-Inference/" />
    <author>
      <name>juhong</name>
    </author>

  
    
    <category term="Model Compression" />
    
    <category term="Quantization" />
    
  

  
    <summary>
      





      1. Neural Network 최적화 연구방향들

1.1.  효율적인 네트워크 설계

Micro-architecture 관점에서는 kernel type을 depth-wise convolution 혹은 low-rank factorization을 사용하는 방법이 있고, Macro-architecture 관점에서는 residual, inception 같은 network를 사용하는 방법이 있다. 이런 방향들의 연구는 대부분 새로운 architecture를 발견하는 것이기 때문에, 작업이 수동적으로 이루어져서 확장가능(Scalable)하지 않다. 그래서 최근에는 Automated Machine Learning (AutoML)이나 Neural Architecture Search (NAS) 처럼 자동으로 효율적인...
    </summary>
  

  </entry>

</feed>


