<!DOCTYPE html>









<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en"
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    

    

  

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="8-bit Optimizers via Block-wise Quantization 정리" />
<meta name="author" content="Juhong Song" />
<meta property="og:locale" content="en" />
<meta name="description" content="1. Introduction" />
<meta property="og:description" content="1. Introduction" />
<link rel="canonical" href="http://localhost:4000/posts/8-bit-optimizer/" />
<meta property="og:url" content="http://localhost:4000/posts/8-bit-optimizer/" />
<meta property="og:site_name" content="Juhong Song" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-29T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="8-bit Optimizers via Block-wise Quantization 정리" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Juhong Song","url":"https://github.com/jhss"},"dateModified":"2023-02-12T23:25:44+09:00","datePublished":"2023-01-29T00:00:00+09:00","description":"1. Introduction","headline":"8-bit Optimizers via Block-wise Quantization 정리","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/8-bit-optimizer/"},"url":"http://localhost:4000/posts/8-bit-optimizer/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>8-bit Optimizers via Block-wise Quantization 정리 | Juhong Song
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="Juhong Song">
<meta name="application-name" content="Juhong Song">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  

    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/style.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  
    <!--
  Switch the mode between dark and light.
-->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get MODE_ATTR() { return "data-mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }
    static get ID() { return "mode-toggle"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener("change", () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();

      });

    } /* constructor() */

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage({
        direction: ModeToggle.ID,
        message: this.modeStatus
      }, "*");
    }

  } /* ModeToggle */

  const toggle = new ModeToggle();

  function flipMode() {
    if (toggle.hasMode) {
      if (toggle.isSysDarkPrefer) {
        if (toggle.isLightMode) {
          toggle.clearMode();
        } else {
          toggle.setLight();
        }

      } else {
        if (toggle.isDarkMode) {
          toggle.clearMode();
        } else {
          toggle.setDark();
        }
      }

    } else {
      if (toggle.isSysDarkPrefer) {
        toggle.setLight();
      } else {
        toggle.setDark();
      }
    }

    toggle.notify();

  } /* flipMode() */

</script>

  
</head>


  <body data-spy="scroll" data-target="#toc" data-topbar-visible="true">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">
  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" class="mx-auto">
        
          
          <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'">
        
      </a>
    </div>

    <div class="site-title">
      <a href="/">Juhong Song</a>
    </div>
    <div class="site-subtitle font-italic"></div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">

    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    
    <li class="nav-item">
      <a href="/categories/" class="nav-link">
        <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>CATEGORIES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/tags/" class="nav-link">
        <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>TAGS</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/archives/" class="nav-link">
        <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ARCHIVES</span>
      </a>
    </li> <!-- .nav-item -->
    
    <li class="nav-item">
      <a href="/about/" class="nav-link">
        <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i>
        

        <span>ABOUT</span>
      </a>
    </li> <!-- .nav-item -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">

    
      <button class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
      <a href="https://github.com/jhss" aria-label="github"
        target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['mtvfc','naver.com'].join('@')" aria-label="email"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper">
  <div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4">
    <span id="breadcrumb">

    

    

      

        
          <span>
            <a href="/">
              Home
            </a>
          </span>

        

      

        

      

        

          
            <span>8-bit Optimizers via Block-wise Quantization 정리</span>
          

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" autocomplete="off" placeholder="Search...">
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div id="main" class="container pl-xl-4 pr-xl-4">
        





<div class="row">

  <!-- core -->
  <div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4">
    <div class="post pl-1 pr-1 pl-md-2 pr-md-2">

    

    
      
      
        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->


<!-- images -->




  
  

  
    
      
      
    

    
    
    

    
    

    
    
    

    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
        

    
      

      <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

      

    

    <!-- Add SVG placehoder to prevent layout reflow -->

    

    <!-- Bypass the HTML-proofer test -->
    

    

  
    

    
    
    

    
    

    
    
    

    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
        

    
      

      <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

      

    

    <!-- Add SVG placehoder to prevent layout reflow -->

    

    <!-- Bypass the HTML-proofer test -->
    

    

  
    

    
    
    

    
    

    
    
    

    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
        

    
      

      <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

      

    

    <!-- Add SVG placehoder to prevent layout reflow -->

    

    <!-- Bypass the HTML-proofer test -->
    

    

  
    

    
    
    

    
    

    
    
    

    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
    
      
      

      
      

      

      
        

    
      

      <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->

      

    

    <!-- Add SVG placehoder to prevent layout reflow -->

    

    <!-- Bypass the HTML-proofer test -->
    

    

  

  



<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    
      

      
      

      
      
      

      

    

    

  

  
  

  

  
  

  




<!-- Wrap prompt element of blockquote with the <div> tag -->







<!-- return -->


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>




<h1 data-toc-skip>8-bit Optimizers via Block-wise Quantization 정리</h1>

<div class="post-meta text-muted">
    <!-- published date -->
    <span>
      Posted
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em class=""
    data-ts="1674918000"
    data-df="ll"
    data-toggle="tooltip" data-placement="bottom">
  Jan 29, 2023
</em>

    </span>

    <!-- lastmod date -->
    
    <span>
      Updated
      <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em class=""
    data-ts="1676211944"
    data-df="ll"
    data-toggle="tooltip" data-placement="bottom">
  Feb 12, 2023
</em>

    </span>
    

  

  <div class="d-flex justify-content-between">
    <!-- author(s) -->
    <span>
      

      By

      <em>
      
        
          <a href="https://github.com/jhss">Juhong Song</a>
          
        
      
      </em>
    </span>

    <div>
      <!-- page views -->
      

      <!-- read time -->
      <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->



<!-- words per minute  -->










<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom"
  title="2348 words">
  <em>13 min</em> read</span>

    </div>

  </div> <!-- .d-flex -->

</div> <!-- .post-meta -->

<div class="post-content">
  <h2 id="1-introduction"><span class="mr-2"><strong>1. Introduction</strong></span><a href="#1-introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>Momentum이나 Adam optimizer는 시간에 따른 gradient 통계량 정보를 이용해서 다음 그레디언트를 업데이트할 때 그레디언트의 방향을 조절하거나 학습률을 조절합니다. 이런 방식은 일반적인 SGD optimizer에 비해서 학습을 가속화 시켜준다는 장점이 있지만, gradient 통계량 정보를 계속 가지고 있어야 하기 때문에 SGD를 사용할 때 보다 더 큰 메모리가 필요하게 됩니다.</p>

\[\begin{gathered}
\\\\
\text { Momentum }\left(\mathbf{g}_t, \mathbf{w}_{t-1}, \mathbf{m}_{t-1}\right)= \begin{cases}\mathbf{m}_0=\mathbf{g}_0 &amp; \text { Initialization } \\
\mathbf{m}_t=\beta_1 \mathbf{m}_{t-1}+\mathbf{g}_t &amp; \text { State 1 update } \\
\mathbf{w}_t=\mathbf{w}_{t-1}-\alpha \cdot \mathbf{m}_t &amp; \text { Weight update }\end{cases} \\\\\\
\text { Adam }\left(\mathbf{g}_t, \mathbf{w}_{t-1}, \mathbf{m}_{t-1}, \mathbf{r}_{t-1}\right)= \begin{cases}\mathbf{r}_0=\mathbf{m}_0=\mathbf{0} &amp; \text { Initialization } \\
\mathbf{m}_t=\beta_1 \mathbf{m}_{t-1}+\left(1-\beta_1\right) \mathbf{g}_t &amp; \text { State 1 update } \\
\mathbf{r}_t=\beta_2 \mathbf{r}_{t-1}+\left(1-\beta_2\right) \mathbf{g}_t^2 &amp; \text { State 2 update } \\
\mathbf{w}_t=\mathbf{w}_{t-1}-\alpha \cdot \frac{\mathbf{m}_t}{\sqrt{\mathbf{r}_t}+\epsilon} &amp; \text { Weight update, }\end{cases}\\\\
\end{gathered}\]

<p>위에 식은 Momentum과 Adam을 사용할 때 state 정보를 사용해서 gradient를 업데이트 하는 과정입니다. 만약에 32 bit로 state를 저장하게 된다면, Momentum은 파라미터 1개 당 4 bytes Adam은 8 bytes가 추가적으로 필요하게 됩니다. 만약에 파라미터 수가 1B정도 된다면 각각 4GB, 8GB만큼 메모리가 추가적으로 필요합니다.</p>

<p>실제로 optimi-zer state는 학습 과정에서 대략 전체 메모리의 33%~75% 정도를 차지한다고 합니다. Largest GPT-2에 대한 optimizer state는 약 11GB, T5에 대한 state는 41GB 정도 된다고 하는데, 이 논문에서는 optimizer state를 8 bit로 저장하는 8-bit optimizer를 통해 학습 과정에서 메모리를 절약할 수 있는 방법을 제시했습니다.</p>

<p>이처럼 quantization을 통해 bit수를 줄여서 학습을 할 때는 크게 3가지 문제가 있습니다. 먼저 적은 bit를 사용했을 때 accuracy가 많이 차이가 나면 안되고 (quantization accuracy), quantization을 적용하는 속도가 빨라야하고 (computational efficiency), 커다란 모델에 대해서도 학습이 안정적으로 이루어져야 합니다 (large-scale stability).</p>

<p>이 논문에서는 위에 3가지 문제를 block-wise quantization을 통해 해결합니다. Block-wise quantization은 입력 tensor를 block으로 나누고, 각 block마다 quantization을 독립적으로 수행하는 방법입니다. 이렇게 block 단위로 나눠서 진행하는 이유는 모든 tensor에 대해 quantization을 수행하면 특정 outlier로 인해서 제대로 안되는 경향이 있는데, block 단위로 나누면 outlier가 block 안에만 한정되니까 학습 안정성과 성능이 증가한다는 장점이 있기 때문입니다.</p>

<p>Block-wise quantization 이외에도 dynamic quantizatino과 stable embedding layer 방법을 추가적으로 제시해서 8-bit optimizer의 안정성과 성능을 향상시켰습니다.</p>

<h2 id="2-background"><span class="mr-2"><strong>2. Background</strong></span><a href="#2-background" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="21-non-linear-quantization"><span class="mr-2"><strong>2.1. Non-linear Quantization</strong></span><a href="#21-non-linear-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Quantization은 precision을 희생하면서 numeric representation을 압축하는 방법입니다. 수식적으로는 \(k\)-bit 정수를 실수로 매핑시켜주는 과정인데, \(\mathbf{Q}^{\text {map }}:\left[0,2^k-1\right] \mapsto D\)으로 표현이 가능합니다. 예를들어 IEEE 32-bit floating point는 \(0 \ldots 2^{32}-1\)를 \([-3.4 \mathrm{e} 38,+3.4 \mathrm{e} 38]\) 으로 대응시킵니다. 일반적인 quantization은 다음과 같은 3단계를 거칩니다.</p>

<hr />
<p><strong>(1)</strong> 입력 tensor \(T\)가 주어졌을 때 표현 가능한 정수 영역으로 대응시키기 위해 normalization constant \(N\)으로 나눠줍니다.</p>

<p><strong>(2)</strong> \(\frac{T}{N}\)과 가장 가까운 값 \(q_i\)을 \(D\)에서 찾습니다.</p>

\[\mathbf{T}_i^Q=\underset{j=0}{\arg \min }\left|\mathbf{Q}_j^{\text {map }}-\frac{\mathbf{T}_i}{N}\right|\]

<p><strong>(3)</strong> \(q_i\)에 해당하는 index \(i\)를 따로 저장합니다.</p>

<hr />

<p>이렇게 qunatization 과정을 거치고, dequantization을 수행할 때는 index를 mapping 함수에 집어넣고 \(N\)을 곱하는 형태로 수행됩니다.</p>

\[\mathbf{T}_i^D=\mathbf{Q}^{\operatorname{map}}\left(\mathbf{T}_i^Q\right) \cdot N\]

<h3 id="22-dynamic-tree-quantization"><span class="mr-2"><strong>2.2. Dynamic Tree Quantization</strong></span><a href="#22-dynamic-tree-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Dynamic quantization은 quantization을 적용하는 값의 크기가 클때나 작을 때 quantization error를 줄이는 기법입니다. 고정된 exponent, fraction을 사용하는 대신에 값에따라 exponent와 fraction을 변화시킵니다.</p>

<p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 300 300'%3E%3C/svg%3E" data-src="http://localhost:4000/assets/img/50.PNG" alt="50.PNG" width="300" height="300" data-proofer-ignore></p>

<p>위의 그림 처럼 4개의 부분으로 나뉘는데, indicator bit를 통해 exponent와 fraction을 조절합니다. Indicator bit는 sign bit이후에 처음으로 1로 세팅되는 bit입니다. 일반적인 linear quantization 방법과 비교해서 quantization error가 낮다는 장점이 있지만, 대상이 되는 값의 범위가 \([-1.0,1.0]\)안에 있어야 해서 absolute max normalization 과정이 필요합니다.</p>

<h2 id="3-proposed-method"><span class="mr-2"><strong>3. Proposed Method</strong></span><a href="#3-proposed-method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>8-Bit optimizer는 block-wise quantization, dynamic quantization, stable embedding layer로 이루어져있습니다. 이 3개의 components를 통해 optimizer state를 8-bit로 저장하고, update를 수행할 때 state를 32-bit로 dequantize한 다음, 저장할 때 다시 8-bit로 quantization을 수행하는 과정으로 진행이 됩니다. 이 과정이 register 안에서 수행되도록 만들었기 때문에 GPU memory copy가 일어나지 않고 추가적으로 임시 메모리도 필요하지 않습니다.</p>

<h3 id="31-block-wise-quantization"><span class="mr-2"><strong>3.1. Block-wise Quantization</strong></span><a href="#31-block-wise-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Block-wise quantization을 통해 outlier를 특정 block에 한정시켜서 normalization 비용을 줄이고 quantization precision을 증가시켰습니다. Dynamic quantization을 수행하기 위해서 tensor의 범위를 \([-1, 1]\)로 맞춰줘야 하는데, 전체 tensor에 대해서 normalize를 한번에 진행하게 되면 core 사이에 synchronization 과정이 수반되기 때문에 속도가 느립니다. 하지만 Block 단위로 normalization을 진행하게 되면 synchronization 과정이 필요가 없어서 이 비용을 줄일 수 있습니다.</p>

<h3 id="32-dynamic-quantization"><span class="mr-2"><strong>3.2. Dynamic Quantization</strong></span><a href="#32-dynamic-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>기존의 Dynamic tree quantization 방법을 확장해서 논문에서 제시한 방법을 dynamic quantization이라고 명명했는데, sign bit를 제거하고 fraction bit를 고정시킨 방법이라고 합니다. Sign bit를 없앤 이유는 Adam optimizer에서 second state가 strictly positive여서 필요하지 않기 때문이고, second Adam state의 magnitude가 3~5 order로 변하기 때문에 fraction bit를 고정시켰다고 합니다. 여기서 의문이 드는건 fraction bit를 고정시키면 더이상 dynamic 형태로 quantization 진행이 안되는거 아닌가? 생각이 들어 그 부분이 아직 이해가가지 않습니다.</p>

<h3 id="33-stable-embedding-layer"><span class="mr-2"><strong>3.3. Stable Embedding Layer</strong></span><a href="#33-stable-embedding-layer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>NLP에서 사용하는 일반적인 Embedding layer와 유사한데, 차이점은 초기화할 때 Xavier uniform 분포를 사용한다는 것과 position embedding을 추가하기 전에 layer normalization을 적용한다는 점입니다. Normal distribution으로 초기화하면 outlier로 인해서 gradient 크기가 커질 수 있기 때문에 학습이 불안정해지고, layer normalization을 통해 variacne를 작게 유지해서 학습의 안정성을 높였습니다.</p>

<h3 id="34-summary"><span class="mr-2"><strong>3.4. Summary</strong></span><a href="#34-summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 700 700'%3E%3C/svg%3E" data-src="http://localhost:4000/assets/img/53.PNG" alt="53.PNG" width="700" height="700" data-proofer-ignore></p>

<p>지금까지 제시한 방법을 정리하면 위와 같습니다. 왼쪽 quantization 과정을 보면 Optimizer state를 block 단위로 나누고, block 안에서 absolute maximum 값을 통해 normalization을 수행합니다. 그 후에 Dynamic quantization을 통해 normalized value와 가장 가까운 8-bit value를 찾습니다. 그렇게 찾은 8-bit value에 대응하는 index를 저장하는 과정을 거칩니다. Dequantization 과정은 앞에서 구한 index를 이용해서 lookup table을 통해 8-bit value를 얻고, denomalization 과정을 거쳐 원래 값을 복원합니다.</p>

<h2 id="4-8-bit-vs-32-bit-optimizer-performance-for-benchmarks"><span class="mr-2"><strong>4. 8-Bit vs 32-Bit Optimizer performance for Benchmarks</strong></span><a href="#4-8-bit-vs-32-bit-optimizer-performance-for-benchmarks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="41-experimental-setup"><span class="mr-2"><strong>4.1. Experimental Setup</strong></span><a href="#41-experimental-setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>실험에서는 Adam, AdamW, Momentum을 기준으로 비교를 했고, 하이퍼파라미터나 weight, gradient, activation의 precision을 변경하지 않은 채 Adam, AdamW, Momentum을 기준으로 비교를 했습니다.</p>

<p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 550 550'%3E%3C/svg%3E" data-src="http://localhost:4000/assets/img/51.PNG" alt="51.PNG" width="550" height="550" data-proofer-ignore></p>

<p>위에 표는 여러 task에서 8-bit optimizer를 사용했을 때 성능이 변하지 않은 채 학습 시간과 모델 크기가 줄어들었다는 것을 보여주는 실험결과입니다. Momentum을 기준으로는 변화가 크지 않지만, Adam을 기준으로 비교했을 때는 학습시간과 필요한 메모리 크기가 꽤 많이 줄어들었다는 것을 볼 수 있습니다.</p>

<h3 id="42-ablation-analysis"><span class="mr-2"><strong>4.2. Ablation Analysis</strong></span><a href="#42-ablation-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="http://localhost:4000/assets/img/52.PNG" alt="52.PNG" width="500" height="500" data-proofer-ignore></p>

<p>작은 언어모델과 큰 언어모델의 perplexity와 학습 안정성을 기준으로 ablation study를 진행했습니다. Stability는 하이퍼 파라미터를 다르게 바꾸면서 학습했을 때 모델의 성능이 어느정도 되는지를 기준으로 측정했습니다. 표를 보시면 작은 스케일 모델에서는 Dynamic quantization, block-wise quantization 영향은 거의 없고, stable embedding을 사용했을 떄 학습 안정성이 증가했습니다. 큰 스케일 모델에서는 Dynamic quantization, Block-wise quantization 영향이 중요하다는 걸 보여주고 있습니다.</p>

<h2 id="5-discussion--limitations"><span class="mr-2"><strong>5. Discussion &amp; Limitations</strong></span><a href="#5-discussion--limitations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>8-bit optimizer는 다양한 task에서 memory 사용량을 낮추고 학습 과정을 가속시키지만, 모델 파라미터에 비례해서 메모리 사용량을 줄이기 때문에 CNN처럼 activation memory를 많이 차지하는 모델에서는 적용하기 어렵습니다. 또한 8-bit optimizer를 NLP에 적용하기 위해서는 논문에서 제시한 stable embedding layer가 필요하다는 단점이 있습니다.</p>

</div>



<div class="post-tail-wrapper text-muted">

  <!-- categories -->
  
  <div class="post-meta mb-3">
    <i class="far fa-folder-open fa-fw mr-1"></i>
    
      <a href='/categories/model-compression/'>Model Compression</a>,
      <a href='/categories/quantization/'>Quantization</a>
  </div>
  

  <!-- tags -->
  
  <div class="post-tags">
    <i class="fa fa-tags fa-fw mr-1"></i>
      
      <a href="/tags/model-compression/"
          class="post-tag no-text-decoration" >Model Compression</a>
      
      <a href="/tags/quantization/"
          class="post-tag no-text-decoration" >Quantization</a>
      
  </div>
  

  <div class="post-tail-bottom
    d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
    <div class="license-wrapper">

      

        

        This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.

      
    </div>

    <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=8-bit+Optimizers+via+Block-wise+Quantization+%EC%A0%95%EB%A6%AC+-+Juhong+Song&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F8-bit-optimizer%2F" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=8-bit+Optimizers+via+Block-wise+Quantization+%EC%A0%95%EB%A6%AC+-+Juhong+Song&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F8-bit-optimizer%2F" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F8-bit-optimizer%2F&text=8-bit+Optimizers+via+Block-wise+Quantization+%EC%A0%95%EB%A6%AC+-+Juhong+Song" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i id="copy-link" class="fa-fw fas fa-link small"
        data-toggle="tooltip" data-placement="top"
        title="Copy link"
        data-title-succeed="Link copied successfully!">
    </i>

  </span>
</div>


  </div><!-- .post-tail-bottom -->

</div><!-- div.post-tail-wrapper -->


	    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */

    var disqus_config = function () {
    this.page.url = 'https://jhss.github.io';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = 'https://jhss.github.io'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://jhss-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




      
    
    

    </div>
  </div> <!-- #core-wrapper -->

  <!-- panel -->
  <div id="panel-wrapper" class="col-xl-3 pl-2 text-muted">

    <div class="access">
      















  <div id="access-lastmod" class="post">
    <div class="panel-heading">Recently Updated</div>
    <ul class="post-content pl-0 pb-1 ml-1 mt-2">
      
        
        
        
      <li><a href="/posts/8-bit-optimizer/">8-bit Optimizers via Block-wise Quantization 정리</a></li>
      
        
        
        
      <li><a href="/posts/8-bit-matrix-multiplication/">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리</a></li>
      
        
        
        
      <li><a href="/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 정리 (Chapter 1 ~ 3)</a></li>
      
        
        
        
      <li><a href="/posts/A-Comprehensive-Survey-on-Graph-Neural-Networks/">A Comprehensive Survey on Graph Neural Networks 정리</a></li>
      
        
        
        
      <li><a href="/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/">Recent Advances on Neural Network Pruning at Initialization 정리</a></li>
      
    </ul>
  </div> <!-- #access-lastmod -->



      















  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/model-compression/">Model Compression</a>
    
      
      <a class="post-tag" href="/tags/quantization/">Quantization</a>
    
      
      <a class="post-tag" href="/tags/pruning/">Pruning</a>
    
      
      <a class="post-tag" href="/tags/graph-neural-network/">Graph Neural Network</a>
    

    </div>
  </div>


    </div>

    
      
      



<!-- BS-toc.js will be loaded at medium priority -->
<script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

<div id="toc-wrapper" class="pl-0 pr-4 mb-5">
  <div class="panel-heading pl-3 pt-2 mb-2">Contents</div>
  <nav id="toc" data-toggle="toc"></nav>
</div>


    
  </div>

</div>

<!-- tail -->

<div class="row">
  <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5">
    
      
      <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->





  <div id="related-posts" class="mb-2 mb-sm-4">
    <h3 class="pt-2 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/posts/A-survey-of-Quantization-Methods-for-Efficient-Neual-Network-Inference/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em class="small"
    data-ts="1671807600"
    data-df="ll"
    >
  Dec 24, 2022
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>A survey of Quantization Methods for Efficient Neural Network 정리</h3>
            <div class="text-muted small">
              <p>
                





                1. Neural Network 최적화 연구방향들

1.1.  효율적인 네트워크 설계

Micro-architecture 관점에서는 kernel type을 depth-wise convolution 혹은 low-rank factorization을 사용하는 방법이 있고, Macro-architecture 관점에서는 residual, inception 같은...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/8-bit-matrix-multiplication/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em class="small"
    data-ts="1676127600"
    data-df="ll"
    >
  Feb 12, 2023
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리</h3>
            <div class="text-muted small">
              <p>
                





                1. Introduction

Large pre-trained 언어모델에 대해서 8-bit quantization을 적용하는 기법들이 많이 연구되었지만, 이런 기법들은 350M 이하 스케일에 대해서만 연구되는 경우가 많았습니다. 이 논문에서는 performance 감소없이 billion 단위에서도 적용 가능한 quantization 기법을 제시합니다....
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/posts/Recent-Advances-on-Neural-Network-Pruning-at-Init/">
          <div class="card-body">
            <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->





<em class="small"
    data-ts="1672498800"
    data-df="ll"
    >
  Jan  1, 2023
</em>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Recent Advances on Neural Network Pruning at Initialization 정리</h3>
            <div class="text-muted small">
              <p>
                





                1. Introduction

기존의 Pruning 기법은 pretrained model에 적용하는 방법이 대부분이었습니다. 하지만 최근에는 임의로 초기화된 네트워크 (a randomly initialized network)에 pruning 기법을 적용하는 방법들이 연구되고 있습니다. 이 기법을 Pruning at Initialization (PaI)...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->


    
      
      <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/posts/Sparsity-in-Deep-Learning-Pruning-and-growth/" class="btn btn-outline-primary"
    prompt="Older">
    <p>Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks 정리 (Chapter 1 ~ 3)</p>
  </a>
  

  
  <a href="/posts/8-bit-matrix-multiplication/" class="btn btn-outline-primary"
    prompt="Newer">
    <p>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 정리</p>
  </a>
  

</div>

    
      
      <!--  The comments switcher -->

  
  <!--
  The Disqus lazy loading.
-->
<div id="disqus_thread" class="pt-2 pb-2">
  <p class="text-center text-muted small">
    Comments powered by <a href="https://disqus.com/">Disqus</a>.
  </p>
</div>

<script type="text/javascript">

  var disqus_config = function () {
    this.page.url = 'http://localhost:4000/posts/8-bit-optimizer/';
    this.page.identifier = '/posts/8-bit-optimizer/';
  };

  /* Lazy loading */
  var disqus_observer = new IntersectionObserver(function (entries) {
    if(entries[0].isIntersecting) {
        (function () {
            var d = document, s = d.createElement('script');
            s.src = 'https://jhss-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();

        disqus_observer.disconnect();
    }
  }, { threshold: [0] });

  disqus_observer.observe(document.querySelector('#disqus_thread'));

  /* Auto switch theme */
  function reloadDisqus() {
    if (event.source === window && event.data &&
      event.data.direction === ModeToggle.ID) {
      /* Disqus hasn't been loaded */
      if (typeof DISQUS === "undefined") {
        return;
      }

      if (document.readyState == 'complete') {
        DISQUS.reset({ reload: true, config: disqus_config });
      }
    }
  }

  const modeToggle = document.querySelector(".mode-toggle");

  if (typeof modeToggle !== "undefined") {
    window.addEventListener("message", reloadDisqus);
  }

</script>



    
  </div>
</div>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      















  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
  
    
    
    
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
        
        



  <div id="access-tags">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">

    
      
      <a class="post-tag" href="/tags/model-compression/">Model Compression</a>
    
      
      <a class="post-tag" href="/tags/quantization/">Quantization</a>
    
      
      <a class="post-tag" href="/tags/pruning/">Pruning</a>
    
      
      <a class="post-tag" href="/tags/graph-neural-network/">Graph Neural Network</a>
    

    </div>
  </div>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    <!-- The Footer -->

<footer>
  <div class="container pl-lg-4 pr-lg-4">
    <div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3">
      <div class="footer-left">
        <p class="mb-0">
          © 2023
          <a href="https://github.com/jhss">juhong</a>.
          
          <span data-toggle="tooltip" data-placement="top"
            title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span>
          
        </p>
      </div>

      <div class="footer-right">
        <p class="mb-0">

          

          

          Powered by 
          <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
           with 
          <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>
           theme.
        </p>
      </div>
    </div>
  </div>
</footer>


    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    
      <div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true"
        data-animation="true" data-autohide="false">
        <div class="toast-header">
          <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close">
            <span aria-hidden="true">&times;</span>
          </button>
        </div>
        <div class="toast-body text-center pt-0">
          <p class="pl-2 pr-2 mb-3">A new version of content is available.</p>
          <button type="button" class="btn btn-primary" aria-label="Update">
            Update
          </button>
        </div>
      </div>
    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No results found.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


    <!--
  JS selector for site.
-->

<!-- layout specified -->


  



  <!-- image lazy-loading & popup & clipboard -->
  

  







  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>







  

  

  







  
    

    

  



  
    

    

  



  
    

    

  



  
    

    

  




  <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script>








<script defer src="/assets/js/dist/post.min.js"></script>



<!-- commons -->

<script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>




  </body>

</html>

